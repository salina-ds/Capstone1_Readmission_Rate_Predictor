{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-26T22:41:12.430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'my_stop_words' (list)\n",
      "sucessfully ran function nobtebook!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import color maps\n",
    "import matplotlib.cm as cm\n",
    "# import tokenizer\n",
    "#from nltk import word_tokenize\n",
    "import string\n",
    "# import model related libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, _forest\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, fbeta_score, classification_report, silhouette_samples, silhouette_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn_pandas import DataFrameMapper, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, KBinsDiscretizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "# import a function that convert items into a callable object\n",
    "from operator import itemgetter\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "%matplotlib inline\n",
    "%run functions.ipynb # import my functions from functions notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-26T22:41:13.206Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siulingcheung/Desktop/Capstone1_Readmission_Rate_Predictor/.venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# read in doctor's discharge notes from NOTEEVENTS.csv\n",
    "df_notes = pd.read_csv(\"../data/mimic-iii-clinical-database-1.4/NOTEEVENTS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-26T22:41:13.409Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SUBJECT_ID', 'HADM_ID', 'READMISSION_30DAYS', 'DISCHARGE_LOCATION',\n",
       "       'INSURANCE', 'MARITAL_STATUS', 'GENDER', 'AGE', 'ETHNICITY_GRP',\n",
       "       'CURR_SERVICE', 'TEXT_CL', 'AGE_boxcox_lambda_opt', 'NUM_PRESCRIPTION',\n",
       "       'LOS', 'HLOS_CL', 'LOS_RATIO', 'KD', 'HP', 'PUL', 'UT', 'HIV', 'DB',\n",
       "       'MBD', 'TB', 'GA', 'HM', 'HEP', 'HO', 'FR', 'TX', 'LA', 'AF', 'CB',\n",
       "       'PNE', 'HF', 'SP', 'WMCC', 'WCC', 'WOCCMCC', 'WOMCC', 'WCCMCC',\n",
       "       'DRG_SEVERITY', 'DRG_MORTALITY', 'TEXT', 'NUM_PRESCRIPTION_LOG',\n",
       "       'LOS_LOG', 'LOS_boxcox_lambda_opt', 'HLOS_CL_LOG',\n",
       "       'HLOS_CL_boxcox_lambda_opt', 'LOS_RATIO_LOG',\n",
       "       'LOS_RATIO_boxcox_lambda_opt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load master dataframe\n",
    "master = pd.read_csv('../data/master.csv')\n",
    "master.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "master.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this project is not only NLP focused, I will only adopt simple NLP techniques that are appropriate for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(master['TEXT_CL'],master.READMISSION_30DAYS,\n",
    "                                        test_size=0.3,train_size=0.7, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 25.0min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 64.9min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed: 72.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tvec',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_acce...\n",
       "                         'mb__fit_prior': [True, False],\n",
       "                         'tvec__max_features': [1000, 3000],\n",
       "                         'tvec__min_df': [5, 10],\n",
       "                         'tvec__stop_words': [None,\n",
       "                                              ['the', 'and', 'to', 'of', 'was',\n",
       "                                               'with', 'a', 'on', 'in', 'for',\n",
       "                                               'name', 'is', 'patient', 's',\n",
       "                                               'he', 'at', 'as', 'or', 'one',\n",
       "                                               'she', 'his', 'her', 'am',\n",
       "                                               'were', 'you', 'pt', 'pm', 'by',\n",
       "                                               'be', 'had', ...]]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "# Pipeline & Gridsearch setup: TFIDF pipeline setup\n",
    "tvc_pipe = Pipeline([\n",
    " ('tvec', TfidfVectorizer()),\n",
    " ('mb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Setting params for TFIDF Vectorizer gridsearch\n",
    "tf_params = {\n",
    "    'tvec__min_df':[5, 10],\n",
    "    'tvec__max_features':[1000, 3000],\n",
    "    'tvec__stop_words': [None, my_stop_words],\n",
    "    'mb__alpha': np.linspace(0.5, 1.5, 6),\n",
    "    'mb__fit_prior': [True, False], \n",
    "}\n",
    "\n",
    "# Setting up GridSearch for TFIDFVectorizer\n",
    "tvc_gs = GridSearchCV(tvc_pipe, param_grid=tf_params, cv = 3, verbose =1, n_jobs = -1)\n",
    "\n",
    "# Fitting TVC GS\n",
    "tvc_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7313843508168529\n",
      "0.7308463698355395\n"
     ]
    }
   ],
   "source": [
    "print(tvc_gs.score(x_train, y_train))\n",
    "print(tvc_gs.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mb__alpha': 0.5,\n",
       " 'mb__fit_prior': True,\n",
       " 'tvec__max_features': 1000,\n",
       " 'tvec__min_df': 5,\n",
       " 'tvec__stop_words': None}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvc_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer is used instead of CountVectorizer because the former performs better with the return frequency ratio instead of count only\n",
    "# set min_df to 5 to avoid using words that only appeared once or twice\n",
    "vect = TfidfVectorizer(min_df=5, max_features = 3000, lowercase=True, tokenizer=tokenizer_better,\n",
    "                       stop_words=my_stop_words, use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract word features from TEXT\n",
    "feature_words = vect.fit_transform(master['TEXT_CL'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8308, 3000)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_words.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sparse matrix containing terms and document frequencies that are predictive of the positive and negative class\n",
    "neg_doc_matrix = vect.transform(\n",
    "    master[master['READMISSION_30DAYS'] == 0].TEXT_CL)\n",
    "pos_doc_matrix = vect.transform(\n",
    "    master[master['READMISSION_30DAYS'] == 1].TEXT_CL)\n",
    "neg_tf = np.sum(neg_doc_matrix, axis=0)\n",
    "pos_tf = np.sum(pos_doc_matrix, axis=0)\n",
    "neg = np.squeeze(np.asarray(neg_tf))\n",
    "pos = np.squeeze(np.asarray(pos_tf))\n",
    "# combine the spare matrices into a dataframe to derive the total doc_frequencies for each term\n",
    "term_freq_df = pd.DataFrame(\n",
    "    [neg, pos], columns=vect.get_feature_names()).transpose()\n",
    "term_freq_df.columns = ['negative', 'positive']\n",
    "term_freq_df['total'] = term_freq_df['negative'] + term_freq_df['positive']\n",
    "term_freq_df.sort_values(by='total', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a series from the sparse matrix\n",
    "d = pd.Series(term_freq_df.total,\n",
    "              index=term_freq_df.index).sort_values(ascending=False)\n",
    "# plot term frequency for first 50 words\n",
    "ax = d[:50].plot(kind='bar', figsize=(10, 6), width=.8,\n",
    "                 fontsize=14, rot=90, color='b')\n",
    "ax.title.set_size(18)\n",
    "plt.ylabel('count')\n",
    "plt.show()\n",
    "# plot term frequency for 50th to 100th words\n",
    "ax = d[50:100].plot(kind='bar', figsize=(10, 6), width=.8,\n",
    "                    fontsize=14, rot=90, color='g')\n",
    "ax.title.set_size(18)\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building NLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w, X_test_w, y_train, y_test = train_test_split(feature_words,master.READMISSION_30DAYS,\n",
    "                                        test_size=0.3,train_size=0.7, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on training data:  69.554%\n",
      "AUC on test data:  63.707%\n"
     ]
    }
   ],
   "source": [
    "# build and evaluate a Multinomial Naive Bayes model using only word features from discharge notes\n",
    "model = MultinomialNB(alpha = 0.5, fit_prior = True)\n",
    "clf = model.fit(X_train_w, y_train)\n",
    "\n",
    "y_train_preds = clf.predict_proba(X_train_w)[:,1]\n",
    "y_valid_preds = clf.predict_proba(X_test_w)[:,1]\n",
    "\n",
    "auc_train = roc_auc_score(y_train, y_train_preds)\n",
    "auc_valid = roc_auc_score(y_test, y_valid_preds)\n",
    "\n",
    "print (\"AUC on training data: \",\"{0:.3%}\".format(auc_train))\n",
    "print (\"AUC on test data: \",\"{0:.3%}\".format(auc_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the strongly predictive features?\n",
    "\n",
    "Below is a neat trick to identify strongly predictive features (i.e. words). \n",
    "\n",
    "* first, create a data set such that each row has exactly one feature. This is represented by the identity matrix.\n",
    "* use the trained classifier to make predictions on this matrix\n",
    "* sort the rows by predicted probabilities, and pick the top and bottom $K$ rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High probability readmission words\tP(readmit<30 | word)\n",
      "            subdural 0.5704\n",
      "                 sdh 0.5614\n",
      "          herniation 0.4865\n",
      "        neurosurgery 0.4390\n",
      "             lifting 0.4362\n",
      "                 peg 0.4314\n",
      "                  br 0.4312\n",
      "               trach 0.4302\n",
      "           occipital 0.4241\n",
      "        transitional 0.4236\n",
      "Low probability readmission words\tP(readmit<30 | word)\n",
      "             suicide 0.1262\n",
      "               crohn 0.1321\n",
      "         nephrostomy 0.1421\n",
      "         phosphatase 0.1485\n",
      "            alkaline 0.1515\n",
      "                ptca 0.1562\n",
      "     catheterization 0.1569\n",
      "             norvasc 0.1655\n",
      "           verapamil 0.1689\n",
      "             flovent 0.1699\n"
     ]
    }
   ],
   "source": [
    "# plot the top 10 words that are most predictive of the positive and negative class\n",
    "words = np.array(vect.get_feature_names())\n",
    "\n",
    "x = np.eye(X_test_w.shape[1])\n",
    "probs = clf.predict_proba(x)[:, 1]\n",
    "ind = np.argsort(probs) # returned index of probs sorted in ascending order\n",
    "\n",
    "wk_words = words[ind[:10]]\n",
    "strg_words = words[ind[-10:]]\n",
    "\n",
    "wk_prob = probs[ind[:10]]\n",
    "strg_prob = probs[ind[-10:]]\n",
    "\n",
    "zipped = zip(strg_words, strg_prob)\n",
    "strg_zipped = sorted(zipped, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"High probability readmission words\\tP(readmit<30 | word)\")\n",
    "for w, p in strg_zipped:\n",
    "    print(\"{:>20}\".format(w), \"{:.4f}\".format(p))\n",
    "    \n",
    "#zipped = zip(wk_words, wk_prob)\n",
    "#wk_zipped = sorted(zipped, key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "print(\"Low probability readmission words\\tP(readmit<30 | word)\")\n",
    "for w, p in zip(wk_words, wk_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.4f}\".format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "474px",
    "left": "651px",
    "right": "20px",
    "top": "177px",
    "width": "621px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
