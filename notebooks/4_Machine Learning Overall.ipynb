{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning_Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-26T22:41:12.430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'my_stop_words' (list)\n",
      "sucessfully ran function nobtebook!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import color maps\n",
    "import matplotlib.cm as cm\n",
    "import string\n",
    "# import model related libraries\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, _forest\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.metrics import roc_auc_score, fbeta_score, classification_report, confusion_matrix, recall_score, accuracy_score, balanced_accuracy_score\n",
    "from sklearn_pandas import DataFrameMapper, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, KBinsDiscretizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "# import a function that convert items into a callable object\n",
    "from operator import itemgetter\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "%matplotlib inline\n",
    "%run functions.ipynb # import my functions from functions notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-26T22:41:13.409Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SUBJECT_ID', 'HADM_ID', 'READMISSION_30DAYS', 'DISCHARGE_LOCATION',\n",
       "       'INSURANCE', 'MARITAL_STATUS', 'GENDER', 'AGE', 'ETHNICITY_GRP',\n",
       "       'CURR_SERVICE', 'NUM_PRESCRIPTION', 'LOS', 'HLOS_CL', 'LOS_RATIO', 'KD',\n",
       "       'HP', 'PUL', 'UT', 'HIV', 'DB', 'MBD', 'TB', 'GA', 'HM', 'HEP', 'HO',\n",
       "       'FR', 'TX', 'LA', 'AF', 'CB', 'PNE', 'HF', 'SP', 'WMCC', 'WCC',\n",
       "       'WOCCMCC', 'WOMCC', 'WCCMCC', 'DRG_SEVERITY', 'DRG_MORTALITY', 'TEXT',\n",
       "       'TEXT_CL', 'AGE_boxcox_lambda_opt', 'NUM_PRESCRIPTION_LOG', 'LOS_LOG',\n",
       "       'LOS_boxcox_lambda_opt', 'HLOS_CL_LOG', 'HLOS_CL_boxcox_lambda_opt',\n",
       "       'LOS_RATIO_LOG', 'LOS_RATIO_boxcox_lambda_opt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load master dataframe with features ready to be transformed and used by the model\n",
    "master = pd.read_csv('../data/master.csv')\n",
    "master.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "master.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform features using pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a preparation for building and evaluating the final model, all features need to be transformed and combined into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-26T22:41:34.630Z"
    }
   },
   "outputs": [],
   "source": [
    "# using datapipeline tools DataFrameMapper to map each feature to the appropriate format before using it in the model\n",
    "mapper = DataFrameMapper([\n",
    "    ('TEXT', TfidfVectorizer(min_df=10, max_features=3000, lowercase=True, #ngram_range=(2, 2),\n",
    "                             tokenizer=tokenizer_better, stop_words=my_stop_words, use_idf=True))\n",
    "    ,('DISCHARGE_LOCATION', LabelEncoder())\n",
    "    ,('INSURANCE',          LabelEncoder())\n",
    "    ,('MARITAL_STATUS',     LabelEncoder())\n",
    "    ,('ETHNICITY_GRP',      LabelEncoder())\n",
    "    ,('CURR_SERVICE',       LabelEncoder())\n",
    "    ,('GENDER',             LabelEncoder())\n",
    "    ,('AGE_boxcox_lambda_opt',        None)\n",
    "#    ,(['AGE'], KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile'))\n",
    "    ,('NUM_PRESCRIPTION_LOG', None)\n",
    "#    ,('LOS_LOG',                None)\n",
    "#    ,('LOS_boxcox_lambda_opt', None)\n",
    "#    ,('HLOS_CL_LOG',               None)\n",
    "#    ,('HLOS_CL_boxcox_lambda_opt', None)\n",
    "#    ,(['LOS_RATIO'], KBinsDiscretizer(n_bins=5, encode='onehot', strategy='quantile'))\n",
    "#    ,('LOS_RATIO_LOG', None)\n",
    "    ,('LOS_RATIO_boxcox_lambda_opt', None)\n",
    "    ,('KD',                 None)\n",
    "    ,('HP',                 None)\n",
    "    ,('PUL',                None)\n",
    "    ,('UT',                 None)\n",
    "    ,('HIV',                None)\n",
    "    ,('DB',                 None)\n",
    "    ,('MBD',                None)\n",
    "    ,('TB',                 None)\n",
    "    ,('GA',                 None)\n",
    "    ,('HM',                 None)\n",
    "    ,('HF',                 None)\n",
    "    ,('HEP',                None)\n",
    "    ,('HO',                 None)\n",
    "    ,('FR',                 None)\n",
    "    ,('TX',                 None)\n",
    "    ,('LA',                 None)\n",
    "    ,('AF',                 None)\n",
    "    ,('CB',                 None)\n",
    "    ,('SP',                 None)\n",
    "    ,('PNE',                None)\n",
    "    ,('WMCC',               None)\n",
    "    ,('WCC',                None)\n",
    "    ,('WOCCMCC',            None)\n",
    "    ,('WOMCC',              None)\n",
    "    ,('WCCMCC',             None)\n",
    "#    ,('DRG_SEVERITY',       None)\n",
    "#    ,('DRG_MORTALITY',      None)\n",
    "], df_out=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-26T22:41:34.980Z"
    }
   },
   "outputs": [],
   "source": [
    "# fit and transform the master dataset including discharge notes to return a final feature set\n",
    "set_all = mapper.fit_transform(master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8308, 3034)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TEXT_yourself', 'TEXT_yrs', 'TEXT_zinc', 'TEXT_zofran',\n",
       "       'TEXT_zolpidem', 'TEXT_zosyn', 'DISCHARGE_LOCATION', 'INSURANCE',\n",
       "       'MARITAL_STATUS', 'ETHNICITY_GRP', 'CURR_SERVICE', 'GENDER',\n",
       "       'AGE_boxcox_lambda_opt', 'NUM_PRESCRIPTION_LOG',\n",
       "       'LOS_RATIO_boxcox_lambda_opt', 'KD', 'HP', 'PUL', 'UT', 'HIV', 'DB',\n",
       "       'MBD', 'TB', 'GA', 'HM', 'HF', 'HEP', 'HO', 'FR', 'TX', 'LA', 'AF',\n",
       "       'CB', 'SP', 'PNE', 'WMCC', 'WCC', 'WOCCMCC', 'WOMCC', 'WCCMCC'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_all.columns[-40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size:  5815\n",
      "test size:  2493\n"
     ]
    }
   ],
   "source": [
    "X_train_all, X_valid_all, y_train, y_valid = train_test_split(set_all, master.READMISSION_30DAYS, test_size=0.3)\n",
    "print('training size: ', X_train_all.shape[0])\n",
    "print('test size: ', X_valid_all.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Model performance score:  0.6057\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"stratified\")\n",
    "dummy_clf.fit(X_train_s3, y_train)\n",
    "print('Dummy Model performance score: ', round(dummy_clf.score(X_train_all, y_train),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the best C-value with L1 penalty with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'C': np.linspace(start = 0, stop = 2, num = 10)}\n",
    "\n",
    "logistic = LogisticRegression(penalty='l1',solver='liblinear')\n",
    "\n",
    "clf = GridSearchCV(logistic, param_grid=params,  scoring = \"roc_auc\", cv = 3, verbose =1, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   47.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 2.0\n"
     ]
    }
   ],
   "source": [
    "best_model = clf.fit(X_train_all, y_train)\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
    "#cv_scores = best_model.cv_results_[best_model.cv_results_['mean_test_score']!='nan']\n",
    "#print('Best avg. AUC :', np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_estimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-45f6b2470beb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'best_estimator' is not defined"
     ]
    }
   ],
   "source": [
    "best_estimator.score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create subsets of features using different C-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression-L1 picked 98 variables and eliminated the other 2936 variables\n",
      "TEXT_subdural      3.736139\n",
      "TEXT_metastatic    3.644777\n",
      "TEXT_campus        3.418166\n",
      "TEXT_trach         1.909928\n",
      "TEXT_peg           1.869254\n",
      "dtype: float64\n",
      "LogisticRegression-L1 picked 188 variables and eliminated the other 2846 variables\n",
      "TEXT_metastatic      4.329391\n",
      "TEXT_subdural        4.050962\n",
      "TEXT_transitional    3.893378\n",
      "TEXT_campus          3.271736\n",
      "TEXT_powder          2.826518\n",
      "dtype: float64\n",
      "LogisticRegression-L1 picked 570 variables and eliminated the other 2464 variables\n",
      "TEXT_transitional    7.977199\n",
      "TEXT_lifting         6.536317\n",
      "TEXT_powder          6.224130\n",
      "TEXT_simple          5.541610\n",
      "TEXT_recommend       5.180892\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X_train_s1, X_valid_s1 = create_feature_subset(c=0.67, X_train=X_train_all, X_valid=X_valid_all, y_train=y_train)\n",
    "X_train_s2, X_valid_s2 = create_feature_subset(c=1.0, X_train=X_train_all, X_valid=X_valid_all, y_train=y_train)\n",
    "X_train_s3, X_valid_s3 = create_feature_subset(c=2.0, X_train=X_train_all, X_valid=X_valid_all, y_train=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best parameters for several classifiers with 3 subset of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further improve model performance, I will tune the parameters of a few tree-base algorithm and linearSVC model with 3 subsets of important features found using logistic regression with L1 penalty with 3 different C-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5815, 98) (2493, 98)\n",
      "(5815, 188) (2493, 188)\n",
      "(5815, 570) (2493, 570)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_s1.shape, X_valid_s1.shape)\n",
    "print(X_train_s2.shape, X_valid_s2.shape)\n",
    "print(X_train_s3.shape, X_valid_s3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'LogisticRegression' : LogisticRegression(penalty = 'l2', random_state = 0)\n",
    "    ,'RandomForestClassifier': RandomForestClassifier()\n",
    "    ,'AdaBoostClassifier': AdaBoostClassifier()\n",
    "    ,'GradientBoostingClassifier': GradientBoostingClassifier()\n",
    "    ,'XGBClassifier': XGBClassifier()\n",
    "#    'SVC': SVC()\n",
    "}\n",
    "\n",
    "params1 = {\n",
    "    'LogisticRegression' : {'C': np.linspace(start = 0, stop = 2, num = 10), 'solver': ['liblinear','lbfgs'] },\n",
    "    'RandomForestClassifier': { 'n_estimators': [50, 99],'max_depth' : [1, 5],'min_samples_split' : [5, 10] },\n",
    "    'AdaBoostClassifier':  { 'n_estimators': [50, 99],'learning_rate': [0.3, 1.0] },\n",
    "    'GradientBoostingClassifier': { 'n_estimators': [75, 99], 'learning_rate': [0.8, 1] },\n",
    "    'XGBClassifier': { 'n_estimators': [50, 99],'learning_rate': [0.05, 0.3, 1.0], 'max_depth': [1, 3], \n",
    "                      'min_child_weight' : [2, 10] }\n",
    "#    'SVC': {'kernel': ['linear'], 'C': [1, 10]},\n",
    "}\n",
    "\n",
    "params2 = {\n",
    "    'LogisticRegression' : {'C': np.linspace(start = 0, stop = 2, num = 10), 'solver': ['liblinear','lbfgs'] },\n",
    "    'RandomForestClassifier': { 'n_estimators': [100, 189],'max_depth' : [1, 5],'min_samples_split' : [5, 10] },\n",
    "    'AdaBoostClassifier':  { 'n_estimators': [100, 189],'learning_rate': [0.3, 1.0] },\n",
    "    'GradientBoostingClassifier': { 'n_estimators': [100, 189], 'learning_rate': [0.8, 1.0]},\n",
    "    'XGBClassifier': { 'n_estimators': [100, 189],'learning_rate': [0.05, 0.3, 1.0], 'max_depth': [1, 3], \n",
    "                      'min_child_weight' : [2, 10]}\n",
    "}\n",
    "params3 = {\n",
    "    'LogisticRegression' : {'C': np.linspace(start = 0, stop = 2, num = 10), 'solver': ['liblinear','lbfgs'] },\n",
    "    'RandomForestClassifier': { 'n_estimators': [285, 570],'max_depth' : [1, 3],'min_samples_split' : [5, 10] },\n",
    "    'AdaBoostClassifier':  { 'n_estimators': [285, 570],'learning_rate': [0.3, 1.0] },\n",
    "    'GradientBoostingClassifier': { 'n_estimators': [285, 570], 'learning_rate': [0.8, 1.0]},\n",
    "    'XGBClassifier': { 'n_estimators': [285, 570],'learning_rate': [0.05, 0.3, 1.0], 'max_depth': [1, 3], \n",
    "                      'min_child_weight' : [2, 10]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for LogisticRegression.\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=2)]: Done  60 out of  60 | elapsed:   10.4s finished\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for RandomForestClassifier.\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  24 out of  24 | elapsed:    6.7s finished\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for AdaBoostClassifier.\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  12 out of  12 | elapsed:   12.7s finished\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for GradientBoostingClassifier.\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  12 out of  12 | elapsed:   32.6s finished\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for XGBClassifier.\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   33.6s\n",
      "[Parallel(n_jobs=2)]: Done  72 out of  72 | elapsed:   54.1s finished\n"
     ]
    }
   ],
   "source": [
    "helper1 = EstimatorSelectionHelper(models, params1)\n",
    "helper1.fit(X_train_s1, y_train, scoring='roc_auc', n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "RandomForestClassifier\n",
      "AdaBoostClassifier\n",
      "GradientBoostingClassifier\n",
      "XGBClassifier\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>min_score</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>max_score</th>\n",
       "      <th>std_score</th>\n",
       "      <th>C</th>\n",
       "      <th>solver</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>min_child_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.667032</td>\n",
       "      <td>0.669047</td>\n",
       "      <td>0.672535</td>\n",
       "      <td>0.00247682</td>\n",
       "      <td>2</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.665473</td>\n",
       "      <td>0.667682</td>\n",
       "      <td>0.670727</td>\n",
       "      <td>0.00222512</td>\n",
       "      <td>1.77778</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.663513</td>\n",
       "      <td>0.666146</td>\n",
       "      <td>0.669304</td>\n",
       "      <td>0.00239293</td>\n",
       "      <td>1.55556</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.658282</td>\n",
       "      <td>0.663334</td>\n",
       "      <td>0.667534</td>\n",
       "      <td>0.0038251</td>\n",
       "      <td>1.33333</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.655992</td>\n",
       "      <td>0.660797</td>\n",
       "      <td>0.664649</td>\n",
       "      <td>0.00359776</td>\n",
       "      <td>1.11111</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.651291</td>\n",
       "      <td>0.65692</td>\n",
       "      <td>0.660427</td>\n",
       "      <td>0.00402059</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.64927</td>\n",
       "      <td>0.656768</td>\n",
       "      <td>0.66935</td>\n",
       "      <td>0.00895077</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.644177</td>\n",
       "      <td>0.655737</td>\n",
       "      <td>0.662613</td>\n",
       "      <td>0.00822273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.651395</td>\n",
       "      <td>0.654055</td>\n",
       "      <td>0.6558</td>\n",
       "      <td>0.00191099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.648287</td>\n",
       "      <td>0.653868</td>\n",
       "      <td>0.662227</td>\n",
       "      <td>0.0060206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.640916</td>\n",
       "      <td>0.653693</td>\n",
       "      <td>0.667646</td>\n",
       "      <td>0.0109441</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.645053</td>\n",
       "      <td>0.65143</td>\n",
       "      <td>0.65463</td>\n",
       "      <td>0.00450916</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.645186</td>\n",
       "      <td>0.651231</td>\n",
       "      <td>0.661649</td>\n",
       "      <td>0.00739797</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.638883</td>\n",
       "      <td>0.650004</td>\n",
       "      <td>0.667967</td>\n",
       "      <td>0.0128213</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.628708</td>\n",
       "      <td>0.649225</td>\n",
       "      <td>0.668538</td>\n",
       "      <td>0.0162829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.640362</td>\n",
       "      <td>0.649206</td>\n",
       "      <td>0.666659</td>\n",
       "      <td>0.0123418</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.635553</td>\n",
       "      <td>0.648307</td>\n",
       "      <td>0.656163</td>\n",
       "      <td>0.00909882</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.628076</td>\n",
       "      <td>0.647626</td>\n",
       "      <td>0.6721</td>\n",
       "      <td>0.0183068</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.628968</td>\n",
       "      <td>0.646479</td>\n",
       "      <td>0.671712</td>\n",
       "      <td>0.0182843</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.634562</td>\n",
       "      <td>0.646052</td>\n",
       "      <td>0.660838</td>\n",
       "      <td>0.0109772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.63081</td>\n",
       "      <td>0.645949</td>\n",
       "      <td>0.673159</td>\n",
       "      <td>0.019281</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.636902</td>\n",
       "      <td>0.645631</td>\n",
       "      <td>0.651283</td>\n",
       "      <td>0.00626129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.626563</td>\n",
       "      <td>0.644628</td>\n",
       "      <td>0.654834</td>\n",
       "      <td>0.0128102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.634925</td>\n",
       "      <td>0.643395</td>\n",
       "      <td>0.647697</td>\n",
       "      <td>0.00598954</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.629627</td>\n",
       "      <td>0.643036</td>\n",
       "      <td>0.659481</td>\n",
       "      <td>0.0123755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.639874</td>\n",
       "      <td>0.641192</td>\n",
       "      <td>0.643612</td>\n",
       "      <td>0.00171339</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.629341</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.650386</td>\n",
       "      <td>0.00859992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.633028</td>\n",
       "      <td>0.63911</td>\n",
       "      <td>0.645102</td>\n",
       "      <td>0.00492948</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.631123</td>\n",
       "      <td>0.63877</td>\n",
       "      <td>0.64737</td>\n",
       "      <td>0.00666709</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.628917</td>\n",
       "      <td>0.636333</td>\n",
       "      <td>0.648158</td>\n",
       "      <td>0.00845147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.616096</td>\n",
       "      <td>0.636128</td>\n",
       "      <td>0.666131</td>\n",
       "      <td>0.0216096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.627213</td>\n",
       "      <td>0.634914</td>\n",
       "      <td>0.645333</td>\n",
       "      <td>0.00764292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.621694</td>\n",
       "      <td>0.634366</td>\n",
       "      <td>0.652257</td>\n",
       "      <td>0.0130112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.619104</td>\n",
       "      <td>0.632618</td>\n",
       "      <td>0.650464</td>\n",
       "      <td>0.013164</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.616205</td>\n",
       "      <td>0.630859</td>\n",
       "      <td>0.641969</td>\n",
       "      <td>0.0108126</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.62056</td>\n",
       "      <td>0.629563</td>\n",
       "      <td>0.634508</td>\n",
       "      <td>0.00637624</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.612685</td>\n",
       "      <td>0.628186</td>\n",
       "      <td>0.643672</td>\n",
       "      <td>0.0126502</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.612685</td>\n",
       "      <td>0.62773</td>\n",
       "      <td>0.643806</td>\n",
       "      <td>0.0127258</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.610703</td>\n",
       "      <td>0.624407</td>\n",
       "      <td>0.636915</td>\n",
       "      <td>0.0107343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.617935</td>\n",
       "      <td>0.619893</td>\n",
       "      <td>0.623352</td>\n",
       "      <td>0.00245277</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.595713</td>\n",
       "      <td>0.6156</td>\n",
       "      <td>0.633716</td>\n",
       "      <td>0.0155651</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.595713</td>\n",
       "      <td>0.615272</td>\n",
       "      <td>0.633716</td>\n",
       "      <td>0.0155347</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.604149</td>\n",
       "      <td>0.615222</td>\n",
       "      <td>0.627677</td>\n",
       "      <td>0.00965474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.58535</td>\n",
       "      <td>0.614136</td>\n",
       "      <td>0.643371</td>\n",
       "      <td>0.0236894</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.585861</td>\n",
       "      <td>0.609463</td>\n",
       "      <td>0.631649</td>\n",
       "      <td>0.0187196</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.598017</td>\n",
       "      <td>0.606574</td>\n",
       "      <td>0.622749</td>\n",
       "      <td>0.0114433</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.592604</td>\n",
       "      <td>0.605863</td>\n",
       "      <td>0.629422</td>\n",
       "      <td>0.0167028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.583365</td>\n",
       "      <td>0.596896</td>\n",
       "      <td>0.606533</td>\n",
       "      <td>0.00985118</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.590935</td>\n",
       "      <td>0.596443</td>\n",
       "      <td>0.600328</td>\n",
       "      <td>0.00400253</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.585673</td>\n",
       "      <td>0.595444</td>\n",
       "      <td>0.600926</td>\n",
       "      <td>0.00692622</td>\n",
       "      <td>2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.584855</td>\n",
       "      <td>0.595283</td>\n",
       "      <td>0.604415</td>\n",
       "      <td>0.00803792</td>\n",
       "      <td>1.77778</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.585337</td>\n",
       "      <td>0.594847</td>\n",
       "      <td>0.600678</td>\n",
       "      <td>0.00678165</td>\n",
       "      <td>1.11111</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.586995</td>\n",
       "      <td>0.594608</td>\n",
       "      <td>0.599402</td>\n",
       "      <td>0.00544317</td>\n",
       "      <td>1.55556</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.582809</td>\n",
       "      <td>0.593934</td>\n",
       "      <td>0.599839</td>\n",
       "      <td>0.00787142</td>\n",
       "      <td>1.33333</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.583042</td>\n",
       "      <td>0.593206</td>\n",
       "      <td>0.598384</td>\n",
       "      <td>0.00718751</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.583708</td>\n",
       "      <td>0.592397</td>\n",
       "      <td>0.599385</td>\n",
       "      <td>0.00651209</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.584268</td>\n",
       "      <td>0.591844</td>\n",
       "      <td>0.595871</td>\n",
       "      <td>0.00536036</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.572575</td>\n",
       "      <td>0.587909</td>\n",
       "      <td>0.596028</td>\n",
       "      <td>0.0108492</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     estimator min_score mean_score max_score   std_score  \\\n",
       "18          LogisticRegression  0.667032   0.669047  0.672535  0.00247682   \n",
       "16          LogisticRegression  0.665473   0.667682  0.670727  0.00222512   \n",
       "14          LogisticRegression  0.663513   0.666146  0.669304  0.00239293   \n",
       "12          LogisticRegression  0.658282   0.663334  0.667534   0.0038251   \n",
       "10          LogisticRegression  0.655992   0.660797  0.664649  0.00359776   \n",
       "8           LogisticRegression  0.651291    0.65692  0.660427  0.00402059   \n",
       "47               XGBClassifier   0.64927   0.656768   0.66935  0.00895077   \n",
       "27      RandomForestClassifier  0.644177   0.655737  0.662613  0.00822273   \n",
       "26      RandomForestClassifier  0.651395   0.654055    0.6558  0.00191099   \n",
       "25      RandomForestClassifier  0.648287   0.653868  0.662227   0.0060206   \n",
       "45               XGBClassifier  0.640916   0.653693  0.667646   0.0109441   \n",
       "6           LogisticRegression  0.645053    0.65143   0.65463  0.00450916   \n",
       "24      RandomForestClassifier  0.645186   0.651231  0.661649  0.00739797   \n",
       "55               XGBClassifier  0.638883   0.650004  0.667967   0.0128213   \n",
       "49               XGBClassifier  0.628708   0.649225  0.668538   0.0162829   \n",
       "50               XGBClassifier  0.640362   0.649206  0.666659   0.0123418   \n",
       "43               XGBClassifier  0.635553   0.648307  0.656163  0.00909882   \n",
       "48               XGBClassifier  0.628076   0.647626    0.6721   0.0183068   \n",
       "51               XGBClassifier  0.628968   0.646479  0.671712   0.0182843   \n",
       "54               XGBClassifier  0.634562   0.646052  0.660838   0.0109772   \n",
       "53               XGBClassifier   0.63081   0.645949  0.673159    0.019281   \n",
       "29          AdaBoostClassifier  0.636902   0.645631  0.651283  0.00626129   \n",
       "41               XGBClassifier  0.626563   0.644628  0.654834   0.0128102   \n",
       "4           LogisticRegression  0.634925   0.643395  0.647697  0.00598954   \n",
       "31          AdaBoostClassifier  0.629627   0.643036  0.659481   0.0123755   \n",
       "46               XGBClassifier  0.639874   0.641192  0.643612  0.00171339   \n",
       "40               XGBClassifier  0.629341   0.640134  0.650386  0.00859992   \n",
       "44               XGBClassifier  0.633028    0.63911  0.645102  0.00492948   \n",
       "42               XGBClassifier  0.631123    0.63877   0.64737  0.00666709   \n",
       "28          AdaBoostClassifier  0.628917   0.636333  0.648158  0.00845147   \n",
       "52               XGBClassifier  0.616096   0.636128  0.666131   0.0216096   \n",
       "30          AdaBoostClassifier  0.627213   0.634914  0.645333  0.00764292   \n",
       "22      RandomForestClassifier  0.621694   0.634366  0.652257   0.0130112   \n",
       "21      RandomForestClassifier  0.619104   0.632618  0.650464    0.013164   \n",
       "23      RandomForestClassifier  0.616205   0.630859  0.641969   0.0108126   \n",
       "2           LogisticRegression   0.62056   0.629563  0.634508  0.00637624   \n",
       "39               XGBClassifier  0.612685   0.628186  0.643672   0.0126502   \n",
       "37               XGBClassifier  0.612685    0.62773  0.643806   0.0127258   \n",
       "20      RandomForestClassifier  0.610703   0.624407  0.636915   0.0107343   \n",
       "58               XGBClassifier  0.617935   0.619893  0.623352  0.00245277   \n",
       "38               XGBClassifier  0.595713     0.6156  0.633716   0.0155651   \n",
       "36               XGBClassifier  0.595713   0.615272  0.633716   0.0155347   \n",
       "59               XGBClassifier  0.604149   0.615222  0.627677  0.00965474   \n",
       "56               XGBClassifier   0.58535   0.614136  0.643371   0.0236894   \n",
       "57               XGBClassifier  0.585861   0.609463  0.631649   0.0187196   \n",
       "33  GradientBoostingClassifier  0.598017   0.606574  0.622749   0.0114433   \n",
       "32  GradientBoostingClassifier  0.592604   0.605863  0.629422   0.0167028   \n",
       "3           LogisticRegression  0.583365   0.596896  0.606533  0.00985118   \n",
       "5           LogisticRegression  0.590935   0.596443  0.600328  0.00400253   \n",
       "19          LogisticRegression  0.585673   0.595444  0.600926  0.00692622   \n",
       "17          LogisticRegression  0.584855   0.595283  0.604415  0.00803792   \n",
       "11          LogisticRegression  0.585337   0.594847  0.600678  0.00678165   \n",
       "15          LogisticRegression  0.586995   0.594608  0.599402  0.00544317   \n",
       "13          LogisticRegression  0.582809   0.593934  0.599839  0.00787142   \n",
       "9           LogisticRegression  0.583042   0.593206  0.598384  0.00718751   \n",
       "34  GradientBoostingClassifier  0.583708   0.592397  0.599385  0.00651209   \n",
       "7           LogisticRegression  0.584268   0.591844  0.595871  0.00536036   \n",
       "35  GradientBoostingClassifier  0.572575   0.587909  0.596028   0.0108492   \n",
       "0           LogisticRegression       NaN        NaN       NaN         NaN   \n",
       "1           LogisticRegression       NaN        NaN       NaN         NaN   \n",
       "\n",
       "           C     solver max_depth min_samples_split n_estimators  \\\n",
       "18         2  liblinear       NaN               NaN          NaN   \n",
       "16   1.77778  liblinear       NaN               NaN          NaN   \n",
       "14   1.55556  liblinear       NaN               NaN          NaN   \n",
       "12   1.33333  liblinear       NaN               NaN          NaN   \n",
       "10   1.11111  liblinear       NaN               NaN          NaN   \n",
       "8   0.888889  liblinear       NaN               NaN          NaN   \n",
       "47       NaN        NaN         1               NaN           99   \n",
       "27       NaN        NaN         5                10           99   \n",
       "26       NaN        NaN         5                10           50   \n",
       "25       NaN        NaN         5                 5           99   \n",
       "45       NaN        NaN         1               NaN           99   \n",
       "6   0.666667  liblinear       NaN               NaN          NaN   \n",
       "24       NaN        NaN         5                 5           50   \n",
       "55       NaN        NaN         1               NaN           99   \n",
       "49       NaN        NaN         3               NaN           99   \n",
       "50       NaN        NaN         3               NaN           50   \n",
       "43       NaN        NaN         3               NaN           99   \n",
       "48       NaN        NaN         3               NaN           50   \n",
       "51       NaN        NaN         3               NaN           99   \n",
       "54       NaN        NaN         1               NaN           50   \n",
       "53       NaN        NaN         1               NaN           99   \n",
       "29       NaN        NaN       NaN               NaN           99   \n",
       "41       NaN        NaN         3               NaN           99   \n",
       "4   0.444444  liblinear       NaN               NaN          NaN   \n",
       "31       NaN        NaN       NaN               NaN           99   \n",
       "46       NaN        NaN         1               NaN           50   \n",
       "40       NaN        NaN         3               NaN           50   \n",
       "44       NaN        NaN         1               NaN           50   \n",
       "42       NaN        NaN         3               NaN           50   \n",
       "28       NaN        NaN       NaN               NaN           50   \n",
       "52       NaN        NaN         1               NaN           50   \n",
       "30       NaN        NaN       NaN               NaN           50   \n",
       "22       NaN        NaN         1                10           50   \n",
       "21       NaN        NaN         1                 5           99   \n",
       "23       NaN        NaN         1                10           99   \n",
       "2   0.222222  liblinear       NaN               NaN          NaN   \n",
       "39       NaN        NaN         1               NaN           99   \n",
       "37       NaN        NaN         1               NaN           99   \n",
       "20       NaN        NaN         1                 5           50   \n",
       "58       NaN        NaN         3               NaN           50   \n",
       "38       NaN        NaN         1               NaN           50   \n",
       "36       NaN        NaN         1               NaN           50   \n",
       "59       NaN        NaN         3               NaN           99   \n",
       "56       NaN        NaN         3               NaN           50   \n",
       "57       NaN        NaN         3               NaN           99   \n",
       "33       NaN        NaN       NaN               NaN           99   \n",
       "32       NaN        NaN       NaN               NaN           75   \n",
       "3   0.222222      lbfgs       NaN               NaN          NaN   \n",
       "5   0.444444      lbfgs       NaN               NaN          NaN   \n",
       "19         2      lbfgs       NaN               NaN          NaN   \n",
       "17   1.77778      lbfgs       NaN               NaN          NaN   \n",
       "11   1.11111      lbfgs       NaN               NaN          NaN   \n",
       "15   1.55556      lbfgs       NaN               NaN          NaN   \n",
       "13   1.33333      lbfgs       NaN               NaN          NaN   \n",
       "9   0.888889      lbfgs       NaN               NaN          NaN   \n",
       "34       NaN        NaN       NaN               NaN           75   \n",
       "7   0.666667      lbfgs       NaN               NaN          NaN   \n",
       "35       NaN        NaN       NaN               NaN           99   \n",
       "0          0  liblinear       NaN               NaN          NaN   \n",
       "1          0      lbfgs       NaN               NaN          NaN   \n",
       "\n",
       "   learning_rate min_child_weight  \n",
       "18           NaN              NaN  \n",
       "16           NaN              NaN  \n",
       "14           NaN              NaN  \n",
       "12           NaN              NaN  \n",
       "10           NaN              NaN  \n",
       "8            NaN              NaN  \n",
       "47           0.3               10  \n",
       "27           NaN              NaN  \n",
       "26           NaN              NaN  \n",
       "25           NaN              NaN  \n",
       "45           0.3                2  \n",
       "6            NaN              NaN  \n",
       "24           NaN              NaN  \n",
       "55             1               10  \n",
       "49           0.3                2  \n",
       "50           0.3               10  \n",
       "43          0.05               10  \n",
       "48           0.3                2  \n",
       "51           0.3               10  \n",
       "54             1               10  \n",
       "53             1                2  \n",
       "29           0.3              NaN  \n",
       "41          0.05                2  \n",
       "4            NaN              NaN  \n",
       "31             1              NaN  \n",
       "46           0.3               10  \n",
       "40          0.05                2  \n",
       "44           0.3                2  \n",
       "42          0.05               10  \n",
       "28           0.3              NaN  \n",
       "52             1                2  \n",
       "30             1              NaN  \n",
       "22           NaN              NaN  \n",
       "21           NaN              NaN  \n",
       "23           NaN              NaN  \n",
       "2            NaN              NaN  \n",
       "39          0.05               10  \n",
       "37          0.05                2  \n",
       "20           NaN              NaN  \n",
       "58             1               10  \n",
       "38          0.05               10  \n",
       "36          0.05                2  \n",
       "59             1               10  \n",
       "56             1                2  \n",
       "57             1                2  \n",
       "33           0.8              NaN  \n",
       "32           0.8              NaN  \n",
       "3            NaN              NaN  \n",
       "5            NaN              NaN  \n",
       "19           NaN              NaN  \n",
       "17           NaN              NaN  \n",
       "11           NaN              NaN  \n",
       "15           NaN              NaN  \n",
       "13           NaN              NaN  \n",
       "9            NaN              NaN  \n",
       "34             1              NaN  \n",
       "7            NaN              NaN  \n",
       "35             1              NaN  \n",
       "0            NaN              NaN  \n",
       "1            NaN              NaN  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helper1.score_summary(sort_by='mean_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for LogisticRegression.\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=2)]: Done  60 out of  60 | elapsed:   19.7s finished\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for RandomForestClassifier.\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  24 out of  24 | elapsed:   15.0s finished\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for AdaBoostClassifier.\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  12 out of  12 | elapsed:   47.7s finished\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for GradientBoostingClassifier.\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  12 out of  12 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for XGBClassifier.\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=2)]: Done  72 out of  72 | elapsed:  3.5min finished\n"
     ]
    }
   ],
   "source": [
    "helper2 = EstimatorSelectionHelper(models, params2)\n",
    "helper2.fit(X_train_s2, y_train, scoring='roc_auc', n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "RandomForestClassifier\n",
      "AdaBoostClassifier\n",
      "GradientBoostingClassifier\n",
      "XGBClassifier\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>min_score</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>max_score</th>\n",
       "      <th>std_score</th>\n",
       "      <th>C</th>\n",
       "      <th>solver</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>min_child_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.687353</td>\n",
       "      <td>0.689899</td>\n",
       "      <td>0.691196</td>\n",
       "      <td>0.00180065</td>\n",
       "      <td>2</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.685186</td>\n",
       "      <td>0.687652</td>\n",
       "      <td>0.689369</td>\n",
       "      <td>0.00178825</td>\n",
       "      <td>1.77778</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.677458</td>\n",
       "      <td>0.682173</td>\n",
       "      <td>0.684571</td>\n",
       "      <td>0.00333399</td>\n",
       "      <td>1.33333</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.682138</td>\n",
       "      <td>0.682964</td>\n",
       "      <td>0.00083171</td>\n",
       "      <td>1.55556</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.671625</td>\n",
       "      <td>0.677947</td>\n",
       "      <td>0.681391</td>\n",
       "      <td>0.00447587</td>\n",
       "      <td>1.11111</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.664519</td>\n",
       "      <td>0.675089</td>\n",
       "      <td>0.684178</td>\n",
       "      <td>0.00809385</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.67004</td>\n",
       "      <td>0.672732</td>\n",
       "      <td>0.677954</td>\n",
       "      <td>0.00369354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.665809</td>\n",
       "      <td>0.67242</td>\n",
       "      <td>0.676094</td>\n",
       "      <td>0.00468469</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.664013</td>\n",
       "      <td>0.670643</td>\n",
       "      <td>0.682841</td>\n",
       "      <td>0.0086359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.654942</td>\n",
       "      <td>0.667337</td>\n",
       "      <td>0.676734</td>\n",
       "      <td>0.00914561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.657427</td>\n",
       "      <td>0.665465</td>\n",
       "      <td>0.674932</td>\n",
       "      <td>0.0072177</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.65299</td>\n",
       "      <td>0.664971</td>\n",
       "      <td>0.674692</td>\n",
       "      <td>0.0090027</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.653545</td>\n",
       "      <td>0.664891</td>\n",
       "      <td>0.6762</td>\n",
       "      <td>0.00924906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.655434</td>\n",
       "      <td>0.664811</td>\n",
       "      <td>0.672301</td>\n",
       "      <td>0.00701393</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.655556</td>\n",
       "      <td>0.664693</td>\n",
       "      <td>0.669316</td>\n",
       "      <td>0.00646047</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.653511</td>\n",
       "      <td>0.663887</td>\n",
       "      <td>0.680187</td>\n",
       "      <td>0.0116679</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.654915</td>\n",
       "      <td>0.663862</td>\n",
       "      <td>0.670566</td>\n",
       "      <td>0.0065835</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.657835</td>\n",
       "      <td>0.663066</td>\n",
       "      <td>0.669241</td>\n",
       "      <td>0.00470419</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.655198</td>\n",
       "      <td>0.661614</td>\n",
       "      <td>0.671434</td>\n",
       "      <td>0.00705178</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.645406</td>\n",
       "      <td>0.661327</td>\n",
       "      <td>0.670544</td>\n",
       "      <td>0.0113046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.656212</td>\n",
       "      <td>0.661286</td>\n",
       "      <td>0.66779</td>\n",
       "      <td>0.00483409</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.655782</td>\n",
       "      <td>0.659666</td>\n",
       "      <td>0.664027</td>\n",
       "      <td>0.00338283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.650404</td>\n",
       "      <td>0.659086</td>\n",
       "      <td>0.668876</td>\n",
       "      <td>0.00758166</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.657077</td>\n",
       "      <td>0.657832</td>\n",
       "      <td>0.658682</td>\n",
       "      <td>0.000658714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.648304</td>\n",
       "      <td>0.657714</td>\n",
       "      <td>0.670532</td>\n",
       "      <td>0.00938919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.652472</td>\n",
       "      <td>0.657666</td>\n",
       "      <td>0.66106</td>\n",
       "      <td>0.00373</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.650297</td>\n",
       "      <td>0.657576</td>\n",
       "      <td>0.664813</td>\n",
       "      <td>0.00592617</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.646454</td>\n",
       "      <td>0.656966</td>\n",
       "      <td>0.669642</td>\n",
       "      <td>0.00958939</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.653931</td>\n",
       "      <td>0.655419</td>\n",
       "      <td>0.657863</td>\n",
       "      <td>0.00174171</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.645909</td>\n",
       "      <td>0.654656</td>\n",
       "      <td>0.65905</td>\n",
       "      <td>0.00618493</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.643856</td>\n",
       "      <td>0.652367</td>\n",
       "      <td>0.666538</td>\n",
       "      <td>0.0100875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.64585</td>\n",
       "      <td>0.651007</td>\n",
       "      <td>0.657381</td>\n",
       "      <td>0.00478537</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.637723</td>\n",
       "      <td>0.645152</td>\n",
       "      <td>0.654758</td>\n",
       "      <td>0.0071227</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.636899</td>\n",
       "      <td>0.644197</td>\n",
       "      <td>0.653708</td>\n",
       "      <td>0.00703836</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.639156</td>\n",
       "      <td>0.643073</td>\n",
       "      <td>0.648299</td>\n",
       "      <td>0.00384585</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.63143</td>\n",
       "      <td>0.639167</td>\n",
       "      <td>0.646083</td>\n",
       "      <td>0.00601028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.633162</td>\n",
       "      <td>0.638306</td>\n",
       "      <td>0.648576</td>\n",
       "      <td>0.00726215</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.613918</td>\n",
       "      <td>0.637906</td>\n",
       "      <td>0.65424</td>\n",
       "      <td>0.0173283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.629256</td>\n",
       "      <td>0.636465</td>\n",
       "      <td>0.643854</td>\n",
       "      <td>0.00596064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.626343</td>\n",
       "      <td>0.63623</td>\n",
       "      <td>0.641915</td>\n",
       "      <td>0.00701761</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.61635</td>\n",
       "      <td>0.634522</td>\n",
       "      <td>0.64422</td>\n",
       "      <td>0.0128593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.62393</td>\n",
       "      <td>0.63449</td>\n",
       "      <td>0.645871</td>\n",
       "      <td>0.00897584</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.622553</td>\n",
       "      <td>0.633726</td>\n",
       "      <td>0.644167</td>\n",
       "      <td>0.00883893</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.621511</td>\n",
       "      <td>0.633242</td>\n",
       "      <td>0.644362</td>\n",
       "      <td>0.00933888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.629267</td>\n",
       "      <td>0.632705</td>\n",
       "      <td>0.635163</td>\n",
       "      <td>0.00250474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.614183</td>\n",
       "      <td>0.628076</td>\n",
       "      <td>0.637401</td>\n",
       "      <td>0.0100136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.606119</td>\n",
       "      <td>0.625358</td>\n",
       "      <td>0.644752</td>\n",
       "      <td>0.0157724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.615665</td>\n",
       "      <td>0.625079</td>\n",
       "      <td>0.630089</td>\n",
       "      <td>0.0066612</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.606085</td>\n",
       "      <td>0.624295</td>\n",
       "      <td>0.646904</td>\n",
       "      <td>0.0169523</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.583419</td>\n",
       "      <td>0.596622</td>\n",
       "      <td>0.605536</td>\n",
       "      <td>0.00952521</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.58344</td>\n",
       "      <td>0.596521</td>\n",
       "      <td>0.605939</td>\n",
       "      <td>0.00954323</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.582345</td>\n",
       "      <td>0.595393</td>\n",
       "      <td>0.60193</td>\n",
       "      <td>0.00922625</td>\n",
       "      <td>1.33333</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.580227</td>\n",
       "      <td>0.595223</td>\n",
       "      <td>0.603031</td>\n",
       "      <td>0.0106064</td>\n",
       "      <td>1.55556</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.583094</td>\n",
       "      <td>0.595133</td>\n",
       "      <td>0.603452</td>\n",
       "      <td>0.00871781</td>\n",
       "      <td>1.11111</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.582716</td>\n",
       "      <td>0.594822</td>\n",
       "      <td>0.600979</td>\n",
       "      <td>0.00856087</td>\n",
       "      <td>2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.58378</td>\n",
       "      <td>0.594676</td>\n",
       "      <td>0.602606</td>\n",
       "      <td>0.0079666</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.585031</td>\n",
       "      <td>0.59461</td>\n",
       "      <td>0.599641</td>\n",
       "      <td>0.00677676</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.584343</td>\n",
       "      <td>0.594224</td>\n",
       "      <td>0.601796</td>\n",
       "      <td>0.00731013</td>\n",
       "      <td>1.77778</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     estimator min_score mean_score max_score    std_score  \\\n",
       "18          LogisticRegression  0.687353   0.689899  0.691196   0.00180065   \n",
       "16          LogisticRegression  0.685186   0.687652  0.689369   0.00178825   \n",
       "12          LogisticRegression  0.677458   0.682173  0.684571   0.00333399   \n",
       "14          LogisticRegression     0.681   0.682138  0.682964   0.00083171   \n",
       "10          LogisticRegression  0.671625   0.677947  0.681391   0.00447587   \n",
       "47               XGBClassifier  0.664519   0.675089  0.684178   0.00809385   \n",
       "29          AdaBoostClassifier   0.67004   0.672732  0.677954   0.00369354   \n",
       "8           LogisticRegression  0.665809    0.67242  0.676094   0.00468469   \n",
       "45               XGBClassifier  0.664013   0.670643  0.682841    0.0086359   \n",
       "48               XGBClassifier  0.654942   0.667337  0.676734   0.00914561   \n",
       "49               XGBClassifier  0.657427   0.665465  0.674932    0.0072177   \n",
       "31          AdaBoostClassifier   0.65299   0.664971  0.674692    0.0090027   \n",
       "55               XGBClassifier  0.653545   0.664891    0.6762   0.00924906   \n",
       "43               XGBClassifier  0.655434   0.664811  0.672301   0.00701393   \n",
       "6           LogisticRegression  0.655556   0.664693  0.669316   0.00646047   \n",
       "53               XGBClassifier  0.653511   0.663887  0.680187    0.0116679   \n",
       "50               XGBClassifier  0.654915   0.663862  0.670566    0.0065835   \n",
       "27      RandomForestClassifier  0.657835   0.663066  0.669241   0.00470419   \n",
       "26      RandomForestClassifier  0.655198   0.661614  0.671434   0.00705178   \n",
       "51               XGBClassifier  0.645406   0.661327  0.670544    0.0113046   \n",
       "25      RandomForestClassifier  0.656212   0.661286   0.66779   0.00483409   \n",
       "46               XGBClassifier  0.655782   0.659666  0.664027   0.00338283   \n",
       "41               XGBClassifier  0.650404   0.659086  0.668876   0.00758166   \n",
       "44               XGBClassifier  0.657077   0.657832  0.658682  0.000658714   \n",
       "52               XGBClassifier  0.648304   0.657714  0.670532   0.00938919   \n",
       "28          AdaBoostClassifier  0.652472   0.657666   0.66106      0.00373   \n",
       "24      RandomForestClassifier  0.650297   0.657576  0.664813   0.00592617   \n",
       "54               XGBClassifier  0.646454   0.656966  0.669642   0.00958939   \n",
       "42               XGBClassifier  0.653931   0.655419  0.657863   0.00174171   \n",
       "4           LogisticRegression  0.645909   0.654656   0.65905   0.00618493   \n",
       "30          AdaBoostClassifier  0.643856   0.652367  0.666538    0.0100875   \n",
       "40               XGBClassifier   0.64585   0.651007  0.657381   0.00478537   \n",
       "39               XGBClassifier  0.637723   0.645152  0.654758    0.0071227   \n",
       "37               XGBClassifier  0.636899   0.644197  0.653708   0.00703836   \n",
       "23      RandomForestClassifier  0.639156   0.643073  0.648299   0.00384585   \n",
       "21      RandomForestClassifier   0.63143   0.639167  0.646083   0.00601028   \n",
       "22      RandomForestClassifier  0.633162   0.638306  0.648576   0.00726215   \n",
       "58               XGBClassifier  0.613918   0.637906   0.65424    0.0173283   \n",
       "20      RandomForestClassifier  0.629256   0.636465  0.643854   0.00596064   \n",
       "2           LogisticRegression  0.626343    0.63623  0.641915   0.00701761   \n",
       "57               XGBClassifier   0.61635   0.634522   0.64422    0.0128593   \n",
       "36               XGBClassifier   0.62393    0.63449  0.645871   0.00897584   \n",
       "38               XGBClassifier  0.622553   0.633726  0.644167   0.00883893   \n",
       "59               XGBClassifier  0.621511   0.633242  0.644362   0.00933888   \n",
       "33  GradientBoostingClassifier  0.629267   0.632705  0.635163   0.00250474   \n",
       "32  GradientBoostingClassifier  0.614183   0.628076  0.637401    0.0100136   \n",
       "35  GradientBoostingClassifier  0.606119   0.625358  0.644752    0.0157724   \n",
       "56               XGBClassifier  0.615665   0.625079  0.630089    0.0066612   \n",
       "34  GradientBoostingClassifier  0.606085   0.624295  0.646904    0.0169523   \n",
       "9           LogisticRegression  0.583419   0.596622  0.605536   0.00952521   \n",
       "5           LogisticRegression   0.58344   0.596521  0.605939   0.00954323   \n",
       "13          LogisticRegression  0.582345   0.595393   0.60193   0.00922625   \n",
       "15          LogisticRegression  0.580227   0.595223  0.603031    0.0106064   \n",
       "11          LogisticRegression  0.583094   0.595133  0.603452   0.00871781   \n",
       "19          LogisticRegression  0.582716   0.594822  0.600979   0.00856087   \n",
       "7           LogisticRegression   0.58378   0.594676  0.602606    0.0079666   \n",
       "3           LogisticRegression  0.585031    0.59461  0.599641   0.00677676   \n",
       "17          LogisticRegression  0.584343   0.594224  0.601796   0.00731013   \n",
       "0           LogisticRegression       NaN        NaN       NaN          NaN   \n",
       "1           LogisticRegression       NaN        NaN       NaN          NaN   \n",
       "\n",
       "           C     solver max_depth min_samples_split n_estimators  \\\n",
       "18         2  liblinear       NaN               NaN          NaN   \n",
       "16   1.77778  liblinear       NaN               NaN          NaN   \n",
       "12   1.33333  liblinear       NaN               NaN          NaN   \n",
       "14   1.55556  liblinear       NaN               NaN          NaN   \n",
       "10   1.11111  liblinear       NaN               NaN          NaN   \n",
       "47       NaN        NaN         1               NaN          189   \n",
       "29       NaN        NaN       NaN               NaN          189   \n",
       "8   0.888889  liblinear       NaN               NaN          NaN   \n",
       "45       NaN        NaN         1               NaN          189   \n",
       "48       NaN        NaN         3               NaN          100   \n",
       "49       NaN        NaN         3               NaN          189   \n",
       "31       NaN        NaN       NaN               NaN          189   \n",
       "55       NaN        NaN         1               NaN          189   \n",
       "43       NaN        NaN         3               NaN          189   \n",
       "6   0.666667  liblinear       NaN               NaN          NaN   \n",
       "53       NaN        NaN         1               NaN          189   \n",
       "50       NaN        NaN         3               NaN          100   \n",
       "27       NaN        NaN         5                10          189   \n",
       "26       NaN        NaN         5                10          100   \n",
       "51       NaN        NaN         3               NaN          189   \n",
       "25       NaN        NaN         5                 5          189   \n",
       "46       NaN        NaN         1               NaN          100   \n",
       "41       NaN        NaN         3               NaN          189   \n",
       "44       NaN        NaN         1               NaN          100   \n",
       "52       NaN        NaN         1               NaN          100   \n",
       "28       NaN        NaN       NaN               NaN          100   \n",
       "24       NaN        NaN         5                 5          100   \n",
       "54       NaN        NaN         1               NaN          100   \n",
       "42       NaN        NaN         3               NaN          100   \n",
       "4   0.444444  liblinear       NaN               NaN          NaN   \n",
       "30       NaN        NaN       NaN               NaN          100   \n",
       "40       NaN        NaN         3               NaN          100   \n",
       "39       NaN        NaN         1               NaN          189   \n",
       "37       NaN        NaN         1               NaN          189   \n",
       "23       NaN        NaN         1                10          189   \n",
       "21       NaN        NaN         1                 5          189   \n",
       "22       NaN        NaN         1                10          100   \n",
       "58       NaN        NaN         3               NaN          100   \n",
       "20       NaN        NaN         1                 5          100   \n",
       "2   0.222222  liblinear       NaN               NaN          NaN   \n",
       "57       NaN        NaN         3               NaN          189   \n",
       "36       NaN        NaN         1               NaN          100   \n",
       "38       NaN        NaN         1               NaN          100   \n",
       "59       NaN        NaN         3               NaN          189   \n",
       "33       NaN        NaN       NaN               NaN          189   \n",
       "32       NaN        NaN       NaN               NaN          100   \n",
       "35       NaN        NaN       NaN               NaN          189   \n",
       "56       NaN        NaN         3               NaN          100   \n",
       "34       NaN        NaN       NaN               NaN          100   \n",
       "9   0.888889      lbfgs       NaN               NaN          NaN   \n",
       "5   0.444444      lbfgs       NaN               NaN          NaN   \n",
       "13   1.33333      lbfgs       NaN               NaN          NaN   \n",
       "15   1.55556      lbfgs       NaN               NaN          NaN   \n",
       "11   1.11111      lbfgs       NaN               NaN          NaN   \n",
       "19         2      lbfgs       NaN               NaN          NaN   \n",
       "7   0.666667      lbfgs       NaN               NaN          NaN   \n",
       "3   0.222222      lbfgs       NaN               NaN          NaN   \n",
       "17   1.77778      lbfgs       NaN               NaN          NaN   \n",
       "0          0  liblinear       NaN               NaN          NaN   \n",
       "1          0      lbfgs       NaN               NaN          NaN   \n",
       "\n",
       "   learning_rate min_child_weight  \n",
       "18           NaN              NaN  \n",
       "16           NaN              NaN  \n",
       "12           NaN              NaN  \n",
       "14           NaN              NaN  \n",
       "10           NaN              NaN  \n",
       "47           0.3               10  \n",
       "29           0.3              NaN  \n",
       "8            NaN              NaN  \n",
       "45           0.3                2  \n",
       "48           0.3                2  \n",
       "49           0.3                2  \n",
       "31             1              NaN  \n",
       "55             1               10  \n",
       "43          0.05               10  \n",
       "6            NaN              NaN  \n",
       "53             1                2  \n",
       "50           0.3               10  \n",
       "27           NaN              NaN  \n",
       "26           NaN              NaN  \n",
       "51           0.3               10  \n",
       "25           NaN              NaN  \n",
       "46           0.3               10  \n",
       "41          0.05                2  \n",
       "44           0.3                2  \n",
       "52             1                2  \n",
       "28           0.3              NaN  \n",
       "24           NaN              NaN  \n",
       "54             1               10  \n",
       "42          0.05               10  \n",
       "4            NaN              NaN  \n",
       "30             1              NaN  \n",
       "40          0.05                2  \n",
       "39          0.05               10  \n",
       "37          0.05                2  \n",
       "23           NaN              NaN  \n",
       "21           NaN              NaN  \n",
       "22           NaN              NaN  \n",
       "58             1               10  \n",
       "20           NaN              NaN  \n",
       "2            NaN              NaN  \n",
       "57             1                2  \n",
       "36          0.05                2  \n",
       "38          0.05               10  \n",
       "59             1               10  \n",
       "33           0.8              NaN  \n",
       "32           0.8              NaN  \n",
       "35             1              NaN  \n",
       "56             1                2  \n",
       "34             1              NaN  \n",
       "9            NaN              NaN  \n",
       "5            NaN              NaN  \n",
       "13           NaN              NaN  \n",
       "15           NaN              NaN  \n",
       "11           NaN              NaN  \n",
       "19           NaN              NaN  \n",
       "7            NaN              NaN  \n",
       "3            NaN              NaN  \n",
       "17           NaN              NaN  \n",
       "0            NaN              NaN  \n",
       "1            NaN              NaN  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helper2.score_summary(sort_by='mean_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for LogisticRegression.\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=2)]: Done  60 out of  60 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for RandomForestClassifier.\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  24 out of  24 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for AdaBoostClassifier.\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  12 out of  12 | elapsed:  8.6min finished\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for GradientBoostingClassifier.\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  12 out of  12 | elapsed: 16.1min finished\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for XGBClassifier.\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed: 18.4min\n",
      "[Parallel(n_jobs=2)]: Done  72 out of  72 | elapsed: 27.3min finished\n"
     ]
    }
   ],
   "source": [
    "helper3 = EstimatorSelectionHelper(models, params3)\n",
    "helper3.fit(X_train_s3, y_train, scoring='roc_auc', n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "RandomForestClassifier\n",
      "AdaBoostClassifier\n",
      "GradientBoostingClassifier\n",
      "XGBClassifier\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>min_score</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>max_score</th>\n",
       "      <th>std_score</th>\n",
       "      <th>C</th>\n",
       "      <th>solver</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>min_child_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.722747</td>\n",
       "      <td>0.723122</td>\n",
       "      <td>0.723377</td>\n",
       "      <td>0.000270691</td>\n",
       "      <td>2</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.718373</td>\n",
       "      <td>0.719402</td>\n",
       "      <td>0.720527</td>\n",
       "      <td>0.000881818</td>\n",
       "      <td>1.77778</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.713689</td>\n",
       "      <td>0.715365</td>\n",
       "      <td>0.716874</td>\n",
       "      <td>0.00130547</td>\n",
       "      <td>1.55556</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.707009</td>\n",
       "      <td>0.710224</td>\n",
       "      <td>0.712618</td>\n",
       "      <td>0.00236234</td>\n",
       "      <td>1.33333</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.699355</td>\n",
       "      <td>0.703822</td>\n",
       "      <td>0.706762</td>\n",
       "      <td>0.00321065</td>\n",
       "      <td>1.11111</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.68656</td>\n",
       "      <td>0.696256</td>\n",
       "      <td>0.702549</td>\n",
       "      <td>0.00695679</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>570</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.688913</td>\n",
       "      <td>0.694712</td>\n",
       "      <td>0.697739</td>\n",
       "      <td>0.0041019</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.683633</td>\n",
       "      <td>0.689032</td>\n",
       "      <td>0.695435</td>\n",
       "      <td>0.00486993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>570</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.68485</td>\n",
       "      <td>0.687435</td>\n",
       "      <td>0.690406</td>\n",
       "      <td>0.00228433</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>570</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.672369</td>\n",
       "      <td>0.686682</td>\n",
       "      <td>0.699825</td>\n",
       "      <td>0.0112392</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>570</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.677899</td>\n",
       "      <td>0.685119</td>\n",
       "      <td>0.689806</td>\n",
       "      <td>0.00518018</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.673664</td>\n",
       "      <td>0.684121</td>\n",
       "      <td>0.691132</td>\n",
       "      <td>0.00753621</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>285</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.672279</td>\n",
       "      <td>0.684002</td>\n",
       "      <td>0.693979</td>\n",
       "      <td>0.00894482</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>570</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.675226</td>\n",
       "      <td>0.683489</td>\n",
       "      <td>0.691084</td>\n",
       "      <td>0.0064914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>570</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.677202</td>\n",
       "      <td>0.683083</td>\n",
       "      <td>0.689536</td>\n",
       "      <td>0.00505168</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>285</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.677724</td>\n",
       "      <td>0.681844</td>\n",
       "      <td>0.687558</td>\n",
       "      <td>0.00416964</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>570</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.671726</td>\n",
       "      <td>0.676996</td>\n",
       "      <td>0.684646</td>\n",
       "      <td>0.00553627</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>570</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.668691</td>\n",
       "      <td>0.672934</td>\n",
       "      <td>0.676745</td>\n",
       "      <td>0.00330201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>285</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.671425</td>\n",
       "      <td>0.672524</td>\n",
       "      <td>0.673675</td>\n",
       "      <td>0.000919246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>285</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.668815</td>\n",
       "      <td>0.671219</td>\n",
       "      <td>0.673106</td>\n",
       "      <td>0.00178926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>570</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.661179</td>\n",
       "      <td>0.670337</td>\n",
       "      <td>0.675807</td>\n",
       "      <td>0.00651615</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.661435</td>\n",
       "      <td>0.670311</td>\n",
       "      <td>0.676978</td>\n",
       "      <td>0.00653481</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>285</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.664029</td>\n",
       "      <td>0.668374</td>\n",
       "      <td>0.675508</td>\n",
       "      <td>0.00508443</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>285</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.650642</td>\n",
       "      <td>0.667029</td>\n",
       "      <td>0.678315</td>\n",
       "      <td>0.0118594</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>570</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.656962</td>\n",
       "      <td>0.665334</td>\n",
       "      <td>0.66976</td>\n",
       "      <td>0.0059228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>570</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.659629</td>\n",
       "      <td>0.664305</td>\n",
       "      <td>0.668738</td>\n",
       "      <td>0.00372284</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>285</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.653165</td>\n",
       "      <td>0.664265</td>\n",
       "      <td>0.670856</td>\n",
       "      <td>0.00789487</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>570</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.6589</td>\n",
       "      <td>0.663363</td>\n",
       "      <td>0.666542</td>\n",
       "      <td>0.0032491</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>285</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.651727</td>\n",
       "      <td>0.659578</td>\n",
       "      <td>0.663664</td>\n",
       "      <td>0.00555287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>285</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.656106</td>\n",
       "      <td>0.657472</td>\n",
       "      <td>0.658713</td>\n",
       "      <td>0.00106796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>285</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.652115</td>\n",
       "      <td>0.656986</td>\n",
       "      <td>0.66667</td>\n",
       "      <td>0.00684805</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>285</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.647331</td>\n",
       "      <td>0.655543</td>\n",
       "      <td>0.662714</td>\n",
       "      <td>0.00632295</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>570</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.652303</td>\n",
       "      <td>0.6554</td>\n",
       "      <td>0.661167</td>\n",
       "      <td>0.00408142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>570</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.650511</td>\n",
       "      <td>0.65527</td>\n",
       "      <td>0.66102</td>\n",
       "      <td>0.00434751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>570</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.650346</td>\n",
       "      <td>0.654742</td>\n",
       "      <td>0.662954</td>\n",
       "      <td>0.00581169</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>285</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.650131</td>\n",
       "      <td>0.65473</td>\n",
       "      <td>0.661325</td>\n",
       "      <td>0.00478293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>285</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.648727</td>\n",
       "      <td>0.654612</td>\n",
       "      <td>0.665237</td>\n",
       "      <td>0.00752765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>570</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.64868</td>\n",
       "      <td>0.654502</td>\n",
       "      <td>0.665409</td>\n",
       "      <td>0.00771818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>570</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.643587</td>\n",
       "      <td>0.653736</td>\n",
       "      <td>0.665604</td>\n",
       "      <td>0.00907047</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>285</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.635928</td>\n",
       "      <td>0.651755</td>\n",
       "      <td>0.66351</td>\n",
       "      <td>0.0116229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>285</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.640541</td>\n",
       "      <td>0.651446</td>\n",
       "      <td>0.66005</td>\n",
       "      <td>0.00812911</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>570</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.642089</td>\n",
       "      <td>0.649991</td>\n",
       "      <td>0.659707</td>\n",
       "      <td>0.00730605</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>285</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.644374</td>\n",
       "      <td>0.649579</td>\n",
       "      <td>0.657012</td>\n",
       "      <td>0.0053947</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>285</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.64313</td>\n",
       "      <td>0.649336</td>\n",
       "      <td>0.656723</td>\n",
       "      <td>0.0056119</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>285</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.637017</td>\n",
       "      <td>0.646978</td>\n",
       "      <td>0.652892</td>\n",
       "      <td>0.00708467</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.636735</td>\n",
       "      <td>0.644114</td>\n",
       "      <td>0.651841</td>\n",
       "      <td>0.00617182</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>570</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.636766</td>\n",
       "      <td>0.643193</td>\n",
       "      <td>0.653589</td>\n",
       "      <td>0.00741899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>570</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.636407</td>\n",
       "      <td>0.641794</td>\n",
       "      <td>0.648245</td>\n",
       "      <td>0.00489115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>285</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.631389</td>\n",
       "      <td>0.64052</td>\n",
       "      <td>0.651322</td>\n",
       "      <td>0.00822286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>285</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.58554</td>\n",
       "      <td>0.59888</td>\n",
       "      <td>0.608167</td>\n",
       "      <td>0.00967175</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.584571</td>\n",
       "      <td>0.59762</td>\n",
       "      <td>0.606764</td>\n",
       "      <td>0.00947148</td>\n",
       "      <td>1.55556</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.585201</td>\n",
       "      <td>0.597141</td>\n",
       "      <td>0.60456</td>\n",
       "      <td>0.00852501</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.588853</td>\n",
       "      <td>0.596749</td>\n",
       "      <td>0.601401</td>\n",
       "      <td>0.00561258</td>\n",
       "      <td>1.11111</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.58582</td>\n",
       "      <td>0.596706</td>\n",
       "      <td>0.604587</td>\n",
       "      <td>0.00795051</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.583921</td>\n",
       "      <td>0.595754</td>\n",
       "      <td>0.602767</td>\n",
       "      <td>0.00841483</td>\n",
       "      <td>1.33333</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.585606</td>\n",
       "      <td>0.595493</td>\n",
       "      <td>0.601649</td>\n",
       "      <td>0.00706086</td>\n",
       "      <td>1.77778</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.58444</td>\n",
       "      <td>0.595118</td>\n",
       "      <td>0.600554</td>\n",
       "      <td>0.00755057</td>\n",
       "      <td>2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.584987</td>\n",
       "      <td>0.595012</td>\n",
       "      <td>0.601716</td>\n",
       "      <td>0.00722199</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     estimator min_score mean_score max_score    std_score  \\\n",
       "18          LogisticRegression  0.722747   0.723122  0.723377  0.000270691   \n",
       "16          LogisticRegression  0.718373   0.719402  0.720527  0.000881818   \n",
       "14          LogisticRegression  0.713689   0.715365  0.716874   0.00130547   \n",
       "12          LogisticRegression  0.707009   0.710224  0.712618   0.00236234   \n",
       "10          LogisticRegression  0.699355   0.703822  0.706762   0.00321065   \n",
       "49               XGBClassifier   0.68656   0.696256  0.702549   0.00695679   \n",
       "8           LogisticRegression  0.688913   0.694712  0.697739    0.0041019   \n",
       "47               XGBClassifier  0.683633   0.689032  0.695435   0.00486993   \n",
       "45               XGBClassifier   0.68485   0.687435  0.690406   0.00228433   \n",
       "51               XGBClassifier  0.672369   0.686682  0.699825    0.0112392   \n",
       "6           LogisticRegression  0.677899   0.685119  0.689806   0.00518018   \n",
       "50               XGBClassifier  0.673664   0.684121  0.691132   0.00753621   \n",
       "55               XGBClassifier  0.672279   0.684002  0.693979   0.00894482   \n",
       "53               XGBClassifier  0.675226   0.683489  0.691084    0.0064914   \n",
       "48               XGBClassifier  0.677202   0.683083  0.689536   0.00505168   \n",
       "43               XGBClassifier  0.677724   0.681844  0.687558   0.00416964   \n",
       "41               XGBClassifier  0.671726   0.676996  0.684646   0.00553627   \n",
       "46               XGBClassifier  0.668691   0.672934  0.676745   0.00330201   \n",
       "44               XGBClassifier  0.671425   0.672524  0.673675  0.000919246   \n",
       "33  GradientBoostingClassifier  0.668815   0.671219  0.673106   0.00178926   \n",
       "4           LogisticRegression  0.661179   0.670337  0.675807   0.00651615   \n",
       "54               XGBClassifier  0.661435   0.670311  0.676978   0.00653481   \n",
       "42               XGBClassifier  0.664029   0.668374  0.675508   0.00508443   \n",
       "35  GradientBoostingClassifier  0.650642   0.667029  0.678315    0.0118594   \n",
       "57               XGBClassifier  0.656962   0.665334   0.66976    0.0059228   \n",
       "40               XGBClassifier  0.659629   0.664305  0.668738   0.00372284   \n",
       "29          AdaBoostClassifier  0.653165   0.664265  0.670856   0.00789487   \n",
       "52               XGBClassifier    0.6589   0.663363  0.666542    0.0032491   \n",
       "56               XGBClassifier  0.651727   0.659578  0.663664   0.00555287   \n",
       "32  GradientBoostingClassifier  0.656106   0.657472  0.658713   0.00106796   \n",
       "28          AdaBoostClassifier  0.652115   0.656986   0.66667   0.00684805   \n",
       "31          AdaBoostClassifier  0.647331   0.655543  0.662714   0.00632295   \n",
       "37               XGBClassifier  0.652303     0.6554  0.661167   0.00408142   \n",
       "39               XGBClassifier  0.650511    0.65527   0.66102   0.00434751   \n",
       "26      RandomForestClassifier  0.650346   0.654742  0.662954   0.00581169   \n",
       "24      RandomForestClassifier  0.650131    0.65473  0.661325   0.00478293   \n",
       "25      RandomForestClassifier  0.648727   0.654612  0.665237   0.00752765   \n",
       "27      RandomForestClassifier   0.64868   0.654502  0.665409   0.00771818   \n",
       "30          AdaBoostClassifier  0.643587   0.653736  0.665604   0.00907047   \n",
       "58               XGBClassifier  0.635928   0.651755   0.66351    0.0116229   \n",
       "59               XGBClassifier  0.640541   0.651446   0.66005   0.00812911   \n",
       "34  GradientBoostingClassifier  0.642089   0.649991  0.659707   0.00730605   \n",
       "36               XGBClassifier  0.644374   0.649579  0.657012    0.0053947   \n",
       "38               XGBClassifier   0.64313   0.649336  0.656723    0.0056119   \n",
       "2           LogisticRegression  0.637017   0.646978  0.652892   0.00708467   \n",
       "23      RandomForestClassifier  0.636735   0.644114  0.651841   0.00617182   \n",
       "21      RandomForestClassifier  0.636766   0.643193  0.653589   0.00741899   \n",
       "22      RandomForestClassifier  0.636407   0.641794  0.648245   0.00489115   \n",
       "20      RandomForestClassifier  0.631389    0.64052  0.651322   0.00822286   \n",
       "7           LogisticRegression   0.58554    0.59888  0.608167   0.00967175   \n",
       "15          LogisticRegression  0.584571    0.59762  0.606764   0.00947148   \n",
       "3           LogisticRegression  0.585201   0.597141   0.60456   0.00852501   \n",
       "11          LogisticRegression  0.588853   0.596749  0.601401   0.00561258   \n",
       "5           LogisticRegression   0.58582   0.596706  0.604587   0.00795051   \n",
       "13          LogisticRegression  0.583921   0.595754  0.602767   0.00841483   \n",
       "17          LogisticRegression  0.585606   0.595493  0.601649   0.00706086   \n",
       "19          LogisticRegression   0.58444   0.595118  0.600554   0.00755057   \n",
       "9           LogisticRegression  0.584987   0.595012  0.601716   0.00722199   \n",
       "0           LogisticRegression       NaN        NaN       NaN          NaN   \n",
       "1           LogisticRegression       NaN        NaN       NaN          NaN   \n",
       "\n",
       "           C     solver max_depth min_samples_split n_estimators  \\\n",
       "18         2  liblinear       NaN               NaN          NaN   \n",
       "16   1.77778  liblinear       NaN               NaN          NaN   \n",
       "14   1.55556  liblinear       NaN               NaN          NaN   \n",
       "12   1.33333  liblinear       NaN               NaN          NaN   \n",
       "10   1.11111  liblinear       NaN               NaN          NaN   \n",
       "49       NaN        NaN         3               NaN          570   \n",
       "8   0.888889  liblinear       NaN               NaN          NaN   \n",
       "47       NaN        NaN         1               NaN          570   \n",
       "45       NaN        NaN         1               NaN          570   \n",
       "51       NaN        NaN         3               NaN          570   \n",
       "6   0.666667  liblinear       NaN               NaN          NaN   \n",
       "50       NaN        NaN         3               NaN          285   \n",
       "55       NaN        NaN         1               NaN          570   \n",
       "53       NaN        NaN         1               NaN          570   \n",
       "48       NaN        NaN         3               NaN          285   \n",
       "43       NaN        NaN         3               NaN          570   \n",
       "41       NaN        NaN         3               NaN          570   \n",
       "46       NaN        NaN         1               NaN          285   \n",
       "44       NaN        NaN         1               NaN          285   \n",
       "33       NaN        NaN       NaN               NaN          570   \n",
       "4   0.444444  liblinear       NaN               NaN          NaN   \n",
       "54       NaN        NaN         1               NaN          285   \n",
       "42       NaN        NaN         3               NaN          285   \n",
       "35       NaN        NaN       NaN               NaN          570   \n",
       "57       NaN        NaN         3               NaN          570   \n",
       "40       NaN        NaN         3               NaN          285   \n",
       "29       NaN        NaN       NaN               NaN          570   \n",
       "52       NaN        NaN         1               NaN          285   \n",
       "56       NaN        NaN         3               NaN          285   \n",
       "32       NaN        NaN       NaN               NaN          285   \n",
       "28       NaN        NaN       NaN               NaN          285   \n",
       "31       NaN        NaN       NaN               NaN          570   \n",
       "37       NaN        NaN         1               NaN          570   \n",
       "39       NaN        NaN         1               NaN          570   \n",
       "26       NaN        NaN         3                10          285   \n",
       "24       NaN        NaN         3                 5          285   \n",
       "25       NaN        NaN         3                 5          570   \n",
       "27       NaN        NaN         3                10          570   \n",
       "30       NaN        NaN       NaN               NaN          285   \n",
       "58       NaN        NaN         3               NaN          285   \n",
       "59       NaN        NaN         3               NaN          570   \n",
       "34       NaN        NaN       NaN               NaN          285   \n",
       "36       NaN        NaN         1               NaN          285   \n",
       "38       NaN        NaN         1               NaN          285   \n",
       "2   0.222222  liblinear       NaN               NaN          NaN   \n",
       "23       NaN        NaN         1                10          570   \n",
       "21       NaN        NaN         1                 5          570   \n",
       "22       NaN        NaN         1                10          285   \n",
       "20       NaN        NaN         1                 5          285   \n",
       "7   0.666667      lbfgs       NaN               NaN          NaN   \n",
       "15   1.55556      lbfgs       NaN               NaN          NaN   \n",
       "3   0.222222      lbfgs       NaN               NaN          NaN   \n",
       "11   1.11111      lbfgs       NaN               NaN          NaN   \n",
       "5   0.444444      lbfgs       NaN               NaN          NaN   \n",
       "13   1.33333      lbfgs       NaN               NaN          NaN   \n",
       "17   1.77778      lbfgs       NaN               NaN          NaN   \n",
       "19         2      lbfgs       NaN               NaN          NaN   \n",
       "9   0.888889      lbfgs       NaN               NaN          NaN   \n",
       "0          0  liblinear       NaN               NaN          NaN   \n",
       "1          0      lbfgs       NaN               NaN          NaN   \n",
       "\n",
       "   learning_rate min_child_weight  \n",
       "18           NaN              NaN  \n",
       "16           NaN              NaN  \n",
       "14           NaN              NaN  \n",
       "12           NaN              NaN  \n",
       "10           NaN              NaN  \n",
       "49           0.3                2  \n",
       "8            NaN              NaN  \n",
       "47           0.3               10  \n",
       "45           0.3                2  \n",
       "51           0.3               10  \n",
       "6            NaN              NaN  \n",
       "50           0.3               10  \n",
       "55             1               10  \n",
       "53             1                2  \n",
       "48           0.3                2  \n",
       "43          0.05               10  \n",
       "41          0.05                2  \n",
       "46           0.3               10  \n",
       "44           0.3                2  \n",
       "33           0.8              NaN  \n",
       "4            NaN              NaN  \n",
       "54             1               10  \n",
       "42          0.05               10  \n",
       "35             1              NaN  \n",
       "57             1                2  \n",
       "40          0.05                2  \n",
       "29           0.3              NaN  \n",
       "52             1                2  \n",
       "56             1                2  \n",
       "32           0.8              NaN  \n",
       "28           0.3              NaN  \n",
       "31             1              NaN  \n",
       "37          0.05                2  \n",
       "39          0.05               10  \n",
       "26           NaN              NaN  \n",
       "24           NaN              NaN  \n",
       "25           NaN              NaN  \n",
       "27           NaN              NaN  \n",
       "30             1              NaN  \n",
       "58             1               10  \n",
       "59             1               10  \n",
       "34             1              NaN  \n",
       "36          0.05                2  \n",
       "38          0.05               10  \n",
       "2            NaN              NaN  \n",
       "23           NaN              NaN  \n",
       "21           NaN              NaN  \n",
       "22           NaN              NaN  \n",
       "20           NaN              NaN  \n",
       "7            NaN              NaN  \n",
       "15           NaN              NaN  \n",
       "3            NaN              NaN  \n",
       "11           NaN              NaN  \n",
       "5            NaN              NaN  \n",
       "13           NaN              NaN  \n",
       "17           NaN              NaN  \n",
       "19           NaN              NaN  \n",
       "9            NaN              NaN  \n",
       "0            NaN              NaN  \n",
       "1            NaN              NaN  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helper3.score_summary(sort_by='mean_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model with a mean ROC_AUC score range of 72.24%-72.3% (95% confidence interval) is the Logistic Regression L2 with C = 2.0 and 570 features in set3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on training data:  80.757%\n",
      "AUC on test data:  65.580%\n"
     ]
    }
   ],
   "source": [
    "# logistic regression using subset3 of features, Test AUC score using full feature set is 65.44%\n",
    "clf_lo = LogisticRegression(C=2.0, penalty='l2', solver='liblinear').fit(X_train_s3,y_train)\n",
    "\n",
    "y_train_preds = clf_lo.predict_proba(X_train_s3)[:,1]\n",
    "y_valid_preds = clf_lo.predict_proba(X_valid_s3)[:,1]\n",
    "\n",
    "auc_train = roc_auc_score(y_train, y_train_preds)\n",
    "auc_valid = roc_auc_score(y_valid, y_valid_preds)\n",
    "\n",
    "print (\"AUC on training data: \",\"{0:.3%}\".format(auc_train))\n",
    "print (\"AUC on test data: \",\"{0:.3%}\".format(auc_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on training data:  99.978%\n",
      "AUC on test data:  62.681%\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(max_depth=3, n_estimators=570, learning_rate=0.3, min_child_weight=2).fit(X_train_s3,y_train)\n",
    "\n",
    "y_train_preds = xgb.predict_proba(X_train_s3)[:,1]\n",
    "y_valid_preds = xgb.predict_proba(X_valid_s3)[:,1]\n",
    "\n",
    "auc_train = roc_auc_score(y_train, y_train_preds)\n",
    "auc_valid = roc_auc_score(y_valid, y_valid_preds)\n",
    "\n",
    "print (\"AUC on training data: \",\"{0:.3%}\".format(auc_train))\n",
    "print (\"AUC on test data: \",\"{0:.3%}\".format(auc_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on training data:  100.000%\n",
      "AUC on test data:  60.711%\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=570, learning_rate=0.8).fit(X_train_s3,y_train)\n",
    "\n",
    "y_train_preds = gb.predict_proba(X_train_s3)[:,1]\n",
    "y_valid_preds = gb.predict_proba(X_valid_s3)[:,1]\n",
    "\n",
    "auc_train = roc_auc_score(y_train, y_train_preds)\n",
    "auc_valid = roc_auc_score(y_valid, y_valid_preds)\n",
    "\n",
    "print (\"AUC on training data: \",\"{0:.3%}\".format(auc_train))\n",
    "print (\"AUC on test data: \",\"{0:.3%}\".format(auc_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on training data:  91.646%\n",
      "AUC on test data:  62.886%\n"
     ]
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(n_estimators=570,learning_rate=0.3).fit(X_train_s3,y_train)\n",
    "\n",
    "y_train_preds = ada.predict_proba(X_train_s3)[:,1]\n",
    "y_valid_preds = ada.predict_proba(X_valid_s3)[:,1]\n",
    "\n",
    "auc_train = roc_auc_score(y_train, y_train_preds)\n",
    "auc_valid = roc_auc_score(y_valid, y_valid_preds)\n",
    "\n",
    "print (\"AUC on training data: \",\"{0:.3%}\".format(auc_train))\n",
    "print (\"AUC on test data: \",\"{0:.3%}\".format(auc_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on training data:  72.033%\n",
      "AUC on test data:  65.366%\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=285, max_depth=3, min_samples_split=10).fit(X_train_s3,y_train)\n",
    "\n",
    "y_train_preds = rf.predict_proba(X_train_s3)[:,1]\n",
    "y_valid_preds = rf.predict_proba(X_valid_s3)[:,1]\n",
    "\n",
    "auc_train = roc_auc_score(y_train, y_train_preds)\n",
    "auc_valid = roc_auc_score(y_valid, y_valid_preds)\n",
    "\n",
    "print (\"AUC on training data: \",\"{0:.3%}\".format(auc_train))\n",
    "print (\"AUC on test data: \",\"{0:.3%}\".format(auc_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the threshold probability with the highest F score using LogisticRegression L2 C=2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5815,) (5815,)\n"
     ]
    }
   ],
   "source": [
    "print(y_true.shape,y_valid_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.7602751504729148\n",
      "Balanced accuracy score:  0.5725344600728819\n",
      "Threshold probability with best F-score :  0.25 0.54 beta =  1\n"
     ]
    }
   ],
   "source": [
    "y_true = y_train.reset_index(drop=True)\n",
    "y_valid_class = (y_train_preds >= 0.5).astype(int)\n",
    "thresholds=np.arange(0,1,.01)\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_valid_class)\n",
    "print('Accuracy score: ',accuracy)\n",
    "\n",
    "baccuracy = balanced_accuracy_score(y_true, y_valid_class)\n",
    "print('Balanced accuracy score: ',baccuracy)\n",
    "\n",
    "beta, best_threshold, best_fscore, fscores_thres_pairs, adj_class, fscores = \\\n",
    "                    create_fbeta_score(y_true=y_true, y_valid_preds=y_train_preds, thresholds=thresholds, beta=1)\n",
    " \n",
    "print (\"Threshold probability with best F-score : \",  round(best_threshold,2), \n",
    "       round(best_fscore,2), \"beta = \", beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beta_v1, best_threshold_v1, best_fscore_v1, fscores_thres_pairs_v1, adj_class_v1, fscores_v1= \\\n",
    "            create_fbeta_score(y_true=y_true, y_valid_preds=y_train_preds, thresholds=thresholds, beta=0.5)\n",
    "beta_v2, best_threshold_v2, best_fscore_v2, fscores_thres_pairs_v2, adj_class_v2, fscores_v2 = \\\n",
    "            create_fbeta_score(y_true=y_true, y_valid_preds=y_train_preds, thresholds=thresholds, beta=1)\n",
    "beta_v3, best_threshold_v3, best_fscore_v3, fscores_thres_pairs_v3, adj_class_v3, fscores_v3 = \\\n",
    "            create_fbeta_score(y_true=y_true, y_valid_preds=y_train_preds, thresholds=thresholds, beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta =  0.5  threshold =  0.38  best_fscore =  0.5516472203157173\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.90      0.85      4251\n",
      "         1.0       0.60      0.41      0.49      1564\n",
      "\n",
      "    accuracy                           0.77      5815\n",
      "   macro avg       0.70      0.66      0.67      5815\n",
      "weighted avg       0.75      0.77      0.75      5815\n",
      "\n",
      "tn:  3828 fp:  423 fn:  921 tp:  643 \n",
      "\n",
      "--------------------------------------------------------------------------------------\n",
      "beta =  1  threshold =  0.25  best_fscore =  0.540638442381707\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.61      0.72      4251\n",
      "         1.0       0.42      0.76      0.54      1564\n",
      "\n",
      "    accuracy                           0.65      5815\n",
      "   macro avg       0.65      0.69      0.63      5815\n",
      "weighted avg       0.75      0.65      0.67      5815\n",
      "\n",
      "tn:  2592 fp:  1659 fn:  370 tp:  1194\n",
      "--------------------------------------------------------------------------------------\n",
      "beta =  2  threshold =  0.19  best_fscore =  0.6873786407766991\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.38      0.54      4251\n",
      "         1.0       0.35      0.91      0.50      1564\n",
      "\n",
      "    accuracy                           0.52      5815\n",
      "   macro avg       0.63      0.64      0.52      5815\n",
      "weighted avg       0.76      0.52      0.53      5815\n",
      "\n",
      "tn:  1623 fp:  2628 fn:  148 tp:  1416\n"
     ]
    }
   ],
   "source": [
    "print('beta = ',beta_v1, ' threshold = ',best_threshold_v1, ' best_fscore = ',best_fscore_v1)\n",
    "print(classification_report(y_true, adj_class_v1))\n",
    "\n",
    "tn_v1, fp_v1, fn_v1, tp_v1 = (confusion_matrix(y_true, adj_class_v1).ravel())\n",
    "print('tn: ',tn_v1,'fp: ',fp_v1,'fn: ',fn_v1,'tp: ',tp_v1,'\\n')\n",
    "print('--------------------------------------------------------------------------------------')\n",
    "print('beta = ',beta_v2, ' threshold = ',best_threshold_v2, ' best_fscore = ',best_fscore_v2)\n",
    "print(classification_report(y_true, adj_class_v2))\n",
    "\n",
    "tn_v2, fp_v2, fn_v2, tp_v2 = (confusion_matrix(y_true, adj_class_v2).ravel())\n",
    "print('tn: ',tn_v2,'fp: ',fp_v2,'fn: ',fn_v2,'tp: ',tp_v2)\n",
    "print('--------------------------------------------------------------------------------------')\n",
    "print('beta = ',beta_v3, ' threshold = ',best_threshold_v3, ' best_fscore = ',best_fscore_v3)\n",
    "print(classification_report(y_true, adj_class_v3))\n",
    "\n",
    "tn_v3, fp_v3, fn_v3, tp_v3 = (confusion_matrix(y_true, adj_class_v3).ravel())\n",
    "print('tn: ',tn_v3,'fp: ',fp_v3,'fn: ',fn_v3,'tp: ',tp_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(thresholds, fscores_v1, color='blue')\n",
    "plt.plot(thresholds, fscores_v2, color='red')\n",
    "plt.plot(thresholds, fscores_v3, color='grey')\n",
    "plt.title('F Beta Score Curves')\n",
    "#plt.legend(('F Beta 1', 'F Beta 2'))\n",
    "plt.xlabel('Threshold')\n",
    "plt.axvline(x=best_threshold_v1, color='blue')\n",
    "plt.text(best_threshold_v1,0.13,'Optimal Threshold Beta=0.5', rotation=-90)\n",
    "plt.axvline(x=best_threshold_v2, color='red')\n",
    "plt.text(best_threshold_v2,0.15,'Optimal Threshold Beta=1', rotation=-90)\n",
    "plt.axvline(x=best_threshold_v3, color='grey')\n",
    "plt.text(best_threshold_v3,0.3,'Optimal Threshold Beta=2', rotation=-90)\n",
    "plt.ylabel('F Beta Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_true, adj_class_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(start = 0, stop = 2, num = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "474px",
    "left": "1331px",
    "right": "20px",
    "top": "120px",
    "width": "513px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
