{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "import string\n",
    "import scipy.stats as spstats\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to date function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_date(series):\n",
    "    \"\"\"\n",
    "    Convert Series to a Timestamp\n",
    "    \"\"\"\n",
    "    return pd.to_datetime(series, format='%Y-%m-%d %H:%M:%S', errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(dataset, column):\n",
    "    \"\"\"\n",
    "    Preprocess text by removing special characters, puncuations, change uppercase to lowercase \n",
    "    and replace nan with a space\n",
    "    \"\"\"    \n",
    "    new_col = column + '_CL'\n",
    "    dataset[new_col] = dataset[column].apply(lambda x: str(x).lower())\n",
    "    dataset[new_col] = [re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,_]\", \" \", i) for i in dataset[new_col]] #slower than using str.maketrans\n",
    "    dataset[new_col] = dataset[new_col].str.replace('\\n',' ')\n",
    "    dataset[new_col] = dataset[new_col].str.replace('\\r',' ')\n",
    "    dataset[new_col] = dataset[new_col].fillna(' ')\n",
    "    return dataset[new_col].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Text (enhanced version of word_tokenizer; remove special characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function tokenize the text by replacing punctuation and numbers with spaces and lowercase all words\n",
    "def tokenizer_better(text):   \n",
    "    \"\"\"\n",
    "    Tokenize using nltk.word_tokenize; replace punctuation and numbers with spaces and make all words lowercase\n",
    "    \"\"\"\n",
    "    punc_list = string.punctuation+'0123456789'\n",
    "    t = str.maketrans(dict.fromkeys(punc_list, \" \"))\n",
    "    text = text.lower().translate(t)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'my_stop_words' (list)\n"
     ]
    }
   ],
   "source": [
    "# define stop_words \n",
    "my_stop_words = ['the', 'and', 'to', 'of', 'was', 'with', 'a', 'on', 'in', 'for', 'name',\n",
    "                 'is', 'patient', 's', 'he', 'at', 'as', 'or', 'one', 'she', 'his', 'her', 'am',\n",
    "                 'were', 'you', 'pt', 'pm', 'by', 'be', 'had', 'your', 'this', 'date', 'please', 'days', 'day',\n",
    "                 'from', 'there', 'an', 'that', 'p', 'are', 'have', 'has', 'h', 'but', 'o', 'nameis', \n",
    "                 'namepattern', 'which', 'every', 'also', 'should', 'if', 'it', 'been', 'who', 'during', 'ml', 'mg',\n",
    "                 'c', 'q', 'd', 'X', 'i', 'I', 'b', 'dr', 'ct', 'x', 'w', 'l', 'j', 'e', 'r', 't', 'w', '1','n', 'm','y']\n",
    "%store my_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quantile_binning(dataset, column, bins):\n",
    "    \"\"\"\n",
    "    Create quantile binning using  for specified column\n",
    "    \"\"\"\n",
    "    col = dataset[column].values.reshape(-1, 1) # reshape array before feeding into discretizer\n",
    "\n",
    "    enc = KBinsDiscretizer(n_bins=bins, encode='onehot', strategy='quantile')\n",
    "    new_col = column + '_binned'\n",
    "    dataset[new_col] = enc.fit_transform(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_log_transform(dataset, column, title):\n",
    "    \"\"\"\n",
    "    Create a log-transformed version of the specified column\n",
    "    \"\"\"\n",
    "    logname = column + \"_LOG\" \n",
    "    dataset[logname] = np.log((1+ dataset[column])) # create log-transformed column\n",
    "    \n",
    "    log_mean = np.round(np.mean(dataset[logname]), 2)\n",
    "    fig, ax = plt.subplots()\n",
    "    dataset[logname].hist(bins=30, color='#A9C5D3', \n",
    "                                     edgecolor='black', grid=False) # plot log-transformed distribution\n",
    "    plt.axvline(log_mean, color='r')\n",
    "    ax.set_title(title, \n",
    "                 fontsize=12)\n",
    "    ax.set_xlabel(column + ' (log scale)', fontsize=12)\n",
    "    ax.set_ylabel('Frequency', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_boxcox(dataset, column, title):\n",
    "    \"\"\"\n",
    "    Create and plot boxcox-transformed version of the specified column\n",
    "    \"\"\"\n",
    "    var = np.array(dataset[column])\n",
    "    var_clean = var[~np.isnan(var)]\n",
    "    l, opt_lambda = spstats.boxcox(var_clean)\n",
    "    var_boxcox_name = column + '_boxcox_lambda_opt'\n",
    "    dataset[var_boxcox_name] = spstats.boxcox(\n",
    "        dataset[column],\n",
    "        lmbda=opt_lambda)\n",
    "\n",
    "    var_boxcox_mean = np.round(np.mean(dataset[var_boxcox_name]),2)\n",
    "    fig, ax = plt.subplots()\n",
    "    dataset[var_boxcox_name].hist(bins=30, \n",
    "                         color='#A9C5D3', edgecolor='black', grid=False)\n",
    "    plt.axvline(var_boxcox_mean, color='r')\n",
    "    ax.set_title(title, \n",
    "                 fontsize=12)\n",
    "    ax.set_xlabel(column + ' (Boxâ€“Cox transformed)', fontsize=12)\n",
    "    ax.set_ylabel('Frequency', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_subset(c,X_train,X_valid,y_train):\n",
    "    clf = LogisticRegression(penalty='l1',C=c,solver='liblinear').fit(X_train,y_train)\n",
    "    # create a series containing feature importance\n",
    "    coef = pd.Series(np.squeeze(clf.coef_), index = X_train.columns)\n",
    "    print(\"LogisticRegression-L1 picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" \n",
    "          +  str(sum(coef == 0)) + \" variables\")\n",
    "    print(coef.sort_values(ascending=False).head(5))\n",
    "    # extract names of features with non-zero importance \n",
    "    set = pd.DataFrame(coef[coef!= 0]).reset_index()\n",
    "    set = set['index']\n",
    "    # format features names in order to be used as index to subset from complete feature space\n",
    "    f_name=[]\n",
    "    for i in set:\n",
    "        q=''+ i +''\n",
    "        f_name.append(q)\n",
    "    # subset features with non-zero importance\n",
    "    train_subset = X_train[[c for c in X_train.columns if c in f_name]]  \n",
    "    valid_subset = X_valid[[c for c in X_valid.columns if c in f_name]]\n",
    "    return train_subset, valid_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splits(features, target, test_size, train_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, target, test_size=test_size, train_size=train_size, random_state=0)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator Selection Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EstimatorSelectionHelper:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "   #     if not set(models.keys()).issubset(set(params.keys())):\n",
    "   #         missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "   #         raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv=3, n_jobs=3, verbose=1, scoring='roc_auc', refit=False):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring, refit=refit,\n",
    "                              return_train_score=True)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params,**d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            print(k)\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]        \n",
    "                scores.append(r.reshape(len(params),1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params,all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-beta Score & threshold tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fbeta_score(y_true, y_valid_preds, thresholds, beta): \n",
    "    \"\"\"Calculate fscore for each threshold and store as tuples in a list\"\"\"\n",
    "    fscores_thres = []\n",
    "    fscores = []\n",
    "    for threshold in thresholds:\n",
    "        y_valid_class = (y_valid_preds >= threshold).astype(int)\n",
    "        fscore = fbeta_score(y_true, y_valid_class, pos_label=1, beta=beta)\n",
    "        fscores.append(fscore)\n",
    "        fscores_thres.append((threshold, fscore))\n",
    "    \n",
    "    best_threshold = max(fscores_thres, key = itemgetter(1))[0] \n",
    "    best_fscore = max(fscores_thres, key = itemgetter(1))[1] \n",
    "    adj_y_valid_class = (y_valid_preds >= best_threshold).astype(int)\n",
    "    return beta, best_threshold, best_fscore, fscores_thres, adj_y_valid_class, fscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sucessfully ran function nobtebook!\n"
     ]
    }
   ],
   "source": [
    "print('sucessfully ran function nobtebook!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "460px",
    "left": "1163px",
    "right": "20px",
    "top": "120px",
    "width": "390px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
